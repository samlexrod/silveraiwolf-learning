{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7175eaed-fa64-406a-b366-37708f759622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Kafka Topic Creation & Supply Chain Producer Sumulation Tutorial\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Kafka is a distributed event streaming platform capable of handling real-time data feeds. In this tutorial, we will walk through the process of creating a Kafka topic using Python and the kafka-python library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0eeadd7a-4520-47a0-85cd-219c0de58065",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Install Required Packages\n",
    "---\n",
    "\n",
    "To interact with Kafka in Python, install the kafka-python package:\n",
    "```cmd\n",
    "pip install kafka-python\n",
    "```\n",
    "Ensure you have a running MSK or any other Kafka cluster with a plain text **broker server address** before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90fc3944-4aba-4903-9191-ac53b46690d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Create a Kafka Topic Using Python \n",
    "---\n",
    "\n",
    "Below is a Python script to create a topic in Kafka using KafkaAdminClient. This script establishes a connection to the Kafka broker, defines a new topic with the desired configurations, and executes the creation process using Kafka's administrative client. The script also includes a validation step to ensure that the topic is successfully created before closing the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e556171-e819-48e6-8649-f55eaf290dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "\n",
    "# Define Kafka broker address\n",
    "KAFKA_BROKER = \"b-1.msksilveraiwolfuseast.nqmaxv.c11.kafka.us-east-1.amazonaws.com:9092,b-2.msksilveraiwolfuseast.nqmaxv.c11.kafka.us-east-1.amazonaws.com:9092\"  # <- Update with your broker address\n",
    "\n",
    "# Initialize Kafka Admin Client\n",
    "admin_client = KafkaAdminClient(\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    client_id=\"admin-client\"\n",
    ")\n",
    "\n",
    "# Define new topic\n",
    "num_partitions = 1  # Number of partitions\n",
    "replication_factor = 1  # Adjust based on your cluster setup\n",
    "\n",
    "# Define multiple topics\n",
    "topics = [\n",
    "    NewTopic(name=\"shipment-contents\", num_partitions=num_partitions, replication_factor=replication_factor),\n",
    "    NewTopic(name=\"orders-raw\", num_partitions=num_partitions, replication_factor=replication_factor),\n",
    "    NewTopic(name=\"inventory-updates\", num_partitions=num_partitions, replication_factor=replication_factor),\n",
    "    NewTopic(name=\"shipment-status\", num_partitions=num_partitions, replication_factor=replication_factor),\n",
    "    NewTopic(name=\"customer-feedback\", num_partitions=num_partitions, replication_factor=replication_factor)\n",
    "]\n",
    "\n",
    "# Create topic\n",
    "admin_client.create_topics(new_topics=topics, validate_only=False)\n",
    "\n",
    "print(f\"Topic '{' ,'.join([x.name for x in topics])}' created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f6ea5d4-4435-4c1c-a008-a914da331929",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. List and Verify Topics\n",
    "---\n",
    "To list all available Kafka topics using Python, we use KafkaAdminClient, which connects to the Kafka broker and retrieves the list of existing topics. This is useful for verifying that a topic was successfully created or checking the available topics in a Kafka cluster before performing further operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93aafdd9-14b1-430f-8ade-ec2a041d499b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List topics\n",
    "topics = admin_client.list_topics()\n",
    "print(\"Available topics:\", [x for x in topics if not x.startswith(\"__\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "729abcbf-d56d-4688-8e2e-90786cec3c79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Producing Messages to Kafka\n",
    "---\n",
    "\n",
    "To send messages to Kafka, we can use KafkaProducer, which allows us to publish messages to a topic in a structured format.\n",
    "\n",
    "The following Python script demonstrates how to set up a Kafka producer and generate sample supply chain messages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48919bd6-546c-4d0b-b947-91d1341b9865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "from uuid import uuid4\n",
    "\n",
    "# Setting up the Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "    value_serializer=lambda value: json.dumps(value).encode(\"utf-8\"),\n",
    ")\n",
    "\n",
    "# Function to generate raw order messages\n",
    "def generate_order_message(order_id):\n",
    "    return {\n",
    "        \"order_id\": str(uuid4()),\n",
    "        \"product_id\": f\"P-{random.randint(1000, 1005)}\",\n",
    "        \"quantity\": random.randint(1, 100),\n",
    "        \"event_timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "# Function to generate inventory updates\n",
    "def generate_inventory_update():\n",
    "    return {\n",
    "        \"warehouse_location\": f\"WH-{random.randint(1, 3)}\",\n",
    "        \"product_id\": f\"P-{random.randint(1000, 1005)}\",\n",
    "        \"stock_level\": random.randint(0, 500),\n",
    "        \"event_timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "def generate_shipment_contents(chosen_order):\n",
    "\n",
    "    return {\n",
    "        \"shipment_id\": f\"S-{random.randint(10000, 10005)}\",\n",
    "        \"order_id\": chosen_order.get(\"order_id\"),\n",
    "        \"product_id\": chosen_order.get(\"product_id\"),\n",
    "        \"quantity\": random.randint(1, 10),\n",
    "        \"event_timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "# Function to generate shipment status updates\n",
    "def generate_shipment_status(chosen_order):\n",
    "    return {\n",
    "        \"shipment_id\": f\"S-{random.randint(10000, 10005)}\",\n",
    "        \"order_id\": chosen_order.get(\"order_id\"),\n",
    "        \"current_status\": random.choice([\"in_transit\", \"delivered\", \"out_for_delivery\", \"delayed\"]),\n",
    "        \"event_timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "# Customer feedback mapping\n",
    "rating_mapping = {\n",
    "    \"1\": \"Poor\",\n",
    "    \"2\": \"Below Average\",\n",
    "    \"3\": \"Average\",\n",
    "    \"4\": \"Above Average\",\n",
    "    \"5\": \"Excellent\"\n",
    "}\n",
    "\n",
    "# Function to generate customer feedback\n",
    "def generate_customer_feedback(chosen_order):\n",
    "    rating_number = random.randint(1, 5)\n",
    "    return {\n",
    "        \"order_id\": chosen_order.get(\"order_id\"),\n",
    "        \"customer_id\": f\"C-{random.randint(1000, 2000)}\",\n",
    "        \"rating\": rating_number,\n",
    "        \"comment\": rating_mapping.get(str(rating_number)),\n",
    "        \"event_timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "# Sending messages to different Kafka topics\n",
    "existing_orders = []\n",
    "for i in range(20):\n",
    "\n",
    "    # Generating the order \n",
    "    orders_raw = generate_order_message(i)\n",
    "    existing_orders.append(orders_raw)\n",
    "    producer.send(\"orders-raw\", key=str(i), value=orders_raw)\n",
    "\n",
    "    # Updating the inventory\n",
    "    producer.send(\"inventory-updates\", key=str(i), value=generate_inventory_update())\n",
    "\n",
    "    # Generating shipment contents and status from existing orders\n",
    "    chosen_order = random.choice(existing_orders)\n",
    "    producer.send(\"shipment-contents\", key=str(i), value=generate_shipment_contents(chosen_order))\n",
    "\n",
    "    # Generating shipment status from existing orders\n",
    "    producer.send(\"shipment-status\", key=str(i), value=generate_shipment_status(chosen_order))\n",
    "\n",
    "    # Generating customer feedback from existing orders\n",
    "    producer.send(\"customer-feedback\", key=str(i), value=generate_customer_feedback(chosen_order))\n",
    "    print(f\"Sent messages for iteration {i}\")\n",
    "    # time.sleep(random.randint(1, 5))\n",
    "\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a11f938-9061-4411-82d4-9937fa501e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "> **CONTINUE TO THE NEXT NOTEBOOKS**\n",
    "> - **t2a**: Describes how to stream using Spark\n",
    "> - **t2b**: Describes how to stream using Delta Live Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "998f22b2-c2d0-4156-a379-47c45eb8f709",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Clean Tutorial\n",
    "---\n",
    "\n",
    "> Use this to clean the topics if needed.\n",
    "\n",
    "* Delete MSK Clusters when done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4949ccf-c1a8-4580-b1e1-8a5bec0bbbbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "def input_with_timeout(prompt, timeout):\n",
    "    def inner_input():\n",
    "        nonlocal user_input\n",
    "        user_input = input(prompt)\n",
    "    \n",
    "    user_input = None\n",
    "    thread = threading.Thread(target=inner_input)\n",
    "    thread.start()\n",
    "    thread.join(timeout)\n",
    "    \n",
    "    if thread.is_alive():\n",
    "        print(\"\\nInput timed out. Proceeding without cleanup.\")\n",
    "        return \"n\"\n",
    "    return user_input\n",
    "\n",
    "clean = input_with_timeout(\"Do you want to clean up the topics? (y/n): \", 10)\n",
    "if clean.lower() == \"y\":\n",
    "\n",
    "    # List all topics\n",
    "    topics = admin_client.list_topics()\n",
    "\n",
    "    # Filter system topics\n",
    "    topics = [topic for topic in topics if not topic.startswith(\"__\")]\n",
    "    print(\"Topics to delete:\", topics)\n",
    "\n",
    "    # Delete the topics\n",
    "    admin_client.delete_topics(topics=topics)\n",
    "    print(f\"Topics '{', '.join(topics)}' deleted successfully!\")\n",
    "\n",
    "    # Close client\n",
    "    admin_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d378b63c-fc86-4802-8ec3-4e8a1c188bb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "t1-databricks-msk-client-topic-producer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
