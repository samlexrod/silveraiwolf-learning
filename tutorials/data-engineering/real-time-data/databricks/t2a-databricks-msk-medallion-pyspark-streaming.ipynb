{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "416dedd9-976b-47a3-bfd3-c02e5136d5d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark Streaming from MSK (Manage Streaming for Apache Kafka) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c12be354-e608-40eb-8ad0-7078c3d31dbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Introduction\n",
    "---\n",
    "This tutorial will guide you through the process of connecting to Amazon MSK (Managed Streaming for Apache Kafka) using the Apache Kafka package and Spark.\n",
    "\n",
    "**Explanation:**\n",
    "- Apache Kafka is an open-source distributed event streaming platform used for building real-time data pipelines and streaming applications.\n",
    "- Amazon MSK is a managed Kafka service that makes it easy to run Kafka without needing to manage the underlying infrastructure.\n",
    "\n",
    "**What We Will Cover in This Tutorial**\n",
    "---\n",
    "1. Downloading and Testing the MSK Cluster Connection with Kafka on Terminal\n",
    "2. Creating a Bronze, Silver, and Gold Streaming Pipeline\n",
    "3. Creating a Mockup Producer to Send Messages to MSK\n",
    "4. Watching How the Streaming Tables Stream the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5124f7d-164f-4e03-b8cf-cb34d91969f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Download and Extract Apache Kafka\n",
    "---\n",
    "Before connecting to MSK, download and extract Apache Kafka.\n",
    "\n",
    "```cmd\n",
    "wget -O kafka.tgz https://dlcdn.apache.org/kafka/3.9.0/kafka_2.13-3.9.0.tgz\n",
    "tar -xzf kafka.tgz\n",
    "ls kafka\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- The `wget` command downloads Kafka version 2.6.2 from the Apache archive.\n",
    "- The `tar -xzf` command extracts the downloaded Kafka package.\n",
    "- The `mv` command renames the extracted folder to a consise folder name for easier access.\n",
    "- The `rm` command removes the .tgz zip file since it was already extracted.\n",
    "- The `ls kafka` will show the contents of the kafka package: `LICENSE NOTICE bin, ...`. We will be working with `bin/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c171b232-b4b9-4689-a337-dd474423526e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download and Extract Kafka Installation Files"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://archive.apache.org/dist/kafka/2.6.2/kafka_2.12-2.6.2.tgz\n",
    "!tar -xzf kafka_2.12-2.6.2.tgz\n",
    "!rm -rf kafka\n",
    "!mv kafka_2.12-2.6.2 kafka\n",
    "!rm kafka_2.12-2.6.2.tgz\n",
    "!ls kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70bd3999-8802-4dbb-bbfb-fd6c6ae0429d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Test Connection to MSK\n",
    "---\n",
    "To verify the connection, create and list the existing topics in your Kafka cluster.\n",
    "\n",
    "**Create Topic:**\n",
    "```cmd\n",
    "bin/kafka-topics.sh --create --bootstrap-server <<bootstrap server address>> --replication-factor 1 --partitions 1 --topic <<topic name>>\n",
    "```\n",
    "> Example: `bin/kafka-topics.sh --create --bootstrap-server b-1.mskpublicuseast1clust.dd5y5a.c11.kafka.us-east-1.amazonaws.com:9092,b-2.mskpublicuseast1clust.dd5y5a.c11.kafka.us-east-1.amazonaws.com:9092 --replication-factor 1 --partitions 1 --topic silveraiwolf.product.sale`\n",
    "\n",
    "**List Topics:**\n",
    "```cmd\n",
    "bin/kafka-topics.sh --bootstrap-server <<bootstrap server address>> --list\n",
    "```\n",
    "> Example: `bin/kafka-topics.sh --bootstrap-server b-1.mskpublicuseast1clust.dd5y5a.c11.kafka.us-east-1.amazonaws.com:9092,b-2.mskpublicuseast1clust.dd5y5a.c11.kafka.us-east-1.amazonaws.com:9092 --list`\n",
    "\n",
    "**Explanation:**\n",
    "- Replace `<<bootstrap server address>>` with the actual address of your MSK cluster.\n",
    "- This command lists all available topics in the Kafka cluster, ensuring that the connection is established correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57074fd2-0094-4167-97c9-88ce6cd9b60f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Kafka Topic for Product Sales"
    }
   },
   "outputs": [],
   "source": [
    "!cd kafka && bin/kafka-topics.sh --create --bootstrap-server b-1.msksilveraiwofluseast.eztjay.c11.kafka.us-east-1.amazonaws.com:9092,b-2.msksilveraiwofluseast.eztjay.c11.kafka.us-east-1.amazonaws.com:9092 --replication-factor 1 --partitions 1 --topic supplychain-orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef47b35c-0e83-44bb-b450-a56726c56649",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List All Kafka Topics in the Cluster"
    }
   },
   "outputs": [],
   "source": [
    "!cd kafka && bin/kafka-topics.sh --bootstrap-server b-1.msksilveraiwofluseast.eztjay.c11.kafka.us-east-1.amazonaws.com:9092,b-2.msksilveraiwofluseast.eztjay.c11.kafka.us-east-1.amazonaws.com:9092 --list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7557d20a-2757-4f87-92b8-dfca8a851c59",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delete Kafka Topic for Product Sales"
    }
   },
   "outputs": [],
   "source": [
    "# # Uncomment to delete the topic if needed\n",
    "# !cd kafka && bin/kafka-topics.sh --bootstrap-server b-2.msksilveraiwolfuseast.jvye40.c11.kafka.us-east-1.amazonaws.com:9092,b-1.msksilveraiwolfuseast.jvye40.c11.kafka.us-east-1.amazonaws.com:9092 --delete --topic supplychain-ordersa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01c04058-387c-4438-bf7f-69385496c164",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Create Spark Streaming Application\n",
    "---\n",
    "\n",
    "Here we create a Spark Streaming application, and establish a real-time streaming pipeline to ingest and process data from Kafka into the bronze, silver, and gold layer of a Databricks Green-Blue Medallion architecture. The guide includes commands for creating and listing Kafka topics, and outlines key steps for Kafka stream ingestion, data parsing, transformation, and aggregations.\n",
    "\n",
    "> If there is a need to make changes to streaming tables that require a drop. We must clear the checkpoints with `dbutils.fs.rm(\"/Volumes/green/silver/files/checkpoints/\", True)`. WARNING: This will cause Spark to reprocess old data from the source.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c067c909-4bf4-44ec-8890-e70981caa676",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze Streaming\n",
    "\n",
    "This section establishes a **real-time streaming pipeline** to ingest and process data from **Kafka** into the **bronze layer** of a Databricks **Green-Blue Medallion architecture**.  \n",
    "\n",
    "#### **Key Steps:**\n",
    "1. **Kafka Stream Ingestion:**  \n",
    "   - Reads data from Kafka using **Structured Streaming**.  \n",
    "   - Connects to Kafka brokers (`kafka.bootstrap.servers`).  \n",
    "   - Subscribes to a specified topic (`kafka_topic`).  \n",
    "   - Reads messages from the earliest available offset for completeness.  \n",
    "\n",
    "2. **Data Parsing & Transformation:**  \n",
    "   - Defines a **JSON schema** for structured data extraction.  \n",
    "   - Converts the Kafka message **key and value** from binary to **string format**.  \n",
    "   - Parses the **JSON payload** into structured fields (e.g., `order_id`, `product_id`, `quantity`, `status`, etc.).  \n",
    "   - Retains additional Kafka metadata (e.g., offsets, timestamps) for auditing.  \n",
    "\n",
    "3. **Writing to the Bronze Delta Table:**  \n",
    "   - Streams the parsed data into a **Delta table (`green.bronze.orders`)** in **append mode**.  \n",
    "   - Uses a **checkpoint location** to track progress and ensure fault tolerance.  \n",
    "   - Provides **real-time visibility** of incoming Kafka data.  \n",
    "\n",
    "This ensures that raw **supply chain order data** is reliably captured in the **bronze layer**, ready for further cleansing and enrichment in the **silver layer**. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b6eb991-3cdc-4afb-8f90-981f18a79f5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# In production use a secret for the bootstrap_servers, this is just for demo purposes and it is deleted after each demo\n",
    "kafka_bootstrap_servers = \"b-2.msksilveraiwolfuseast.6im2sm.c11.kafka.us-east-1.amazonaws.com:9092,b-1.msksilveraiwolfuseast.6im2sm.c11.kafka.us-east-1.amazonaws.com:9092\"\n",
    "kafka_topic = \"supplychain-orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f63777a4-7143-47fb-a61c-55f5c89bcb85",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Parse and Store Kafka Stream Data with Spark"
    }
   },
   "outputs": [],
   "source": [
    "# Create a stream to read data from Kafka\n",
    "raw_stream = (spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers)\n",
    "    .option(\"subscribe\", kafka_topic)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load())\n",
    "\n",
    "# Parse the data from Kafka to a human-readable format and cast to JSON\n",
    "other_columns = raw_stream.drop(\"key\", \"value\").columns\n",
    "parsed_stream = raw_stream.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", *other_columns)\n",
    "\n",
    "bronze_query = (parsed_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/green/bronze/files/checkpoints/orders\")\n",
    "    .table(\"green.bronze.orders\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d168dd59-43a4-410b-aa94-7381711c99d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Silver Streaming\n",
    "\n",
    "This section processes raw order data from the **bronze layer** and prepares it for structured analysis in the **silver layer** of the Databricks **Green-Blue Medallion architecture**.  \n",
    "\n",
    "#### **Key Steps:**\n",
    "1. **Schema Definition for JSON Parsing:**  \n",
    "   - Defines a structured schema for the **order data**.  \n",
    "   - Ensures correct data types (e.g., `order_id` as an `Integer`, `product_id` as a `String`).  \n",
    "   - Prepares for structured querying and downstream transformations.  \n",
    "\n",
    "2. **Reading from the Bronze Delta Table:**  \n",
    "   - Streams raw **Kafka-ingested order data** from `green.bronze.orders`.  \n",
    "   - Maintains real-time processing capabilities with **Structured Streaming**.  \n",
    "\n",
    "3. **Data Transformation & Cleansing:**  \n",
    "   - Extracts the JSON payload from the `value` column.  \n",
    "   - Flattens the **nested JSON structure** into individual columns.  \n",
    "   - Renames `order_creation_date` to `event_timestamp` for standardization.  \n",
    "\n",
    "4. **Writing to the Silver Delta Table:**  \n",
    "   - Saves the **cleaned and structured data** into `green.silver.orders`.  \n",
    "   - Uses **append mode** to continuously update the dataset.  \n",
    "   - Implements a **checkpoint location** to track progress and ensure fault tolerance.  \n",
    "\n",
    "This process **standardizes and cleans** order data, making it **ready for analytical processing** in the **gold layer** for reporting and business intelligence. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ae3375-414d-46a0-8372-a240d55148ae",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transform and Store Orders Data with Spark Stream"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Define schema for JSON parsing\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"warehouse_location\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"event_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Read Bronze Table and apply transformations\n",
    "bronze_stram = spark.readStream.format(\"delta\").table(\"green.bronze.orders\")\n",
    "\n",
    "# Applying schema and expanding the schema\n",
    "silver_stream = (bronze_stram\n",
    "    .withColumn(\"value\", from_json(col(\"value\"), schema))\n",
    "    .select(\"value.*\", \"offset\", \"timestamp\")\n",
    ")\n",
    "\n",
    "silver_query = (silver_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/Volumes/green/silver/files/checkpoints/orders\")\n",
    "    .table(\"green.silver.orders\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "130fb739-e3d7-4d4f-9629-a4ade1295fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gold Streaming State Handling\n",
    "\n",
    "In Spark Structured Streaming, a sliding window allows you to perform aggregations over a window of time that slides continuously. When combined with watermarks, it helps manage late data and ensures efficient state management. Similarly, a tumbling window performs aggregations over fixed-size, non-overlapping windows of time, providing a straightforward way to handle time-based data.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "1. **Sliding Window:**: \n",
    "It allows overlapping aggregations, meaning an event may belong to multiple windows.\n",
    "   \n",
    "   For example:\n",
    "   - A new window starts every 10 minutes.\n",
    "   - Every 5 minutes, a new overlapping window starts.\n",
    "   - Each window includes data from the last 10 minutes.\n",
    "   - Events may contribute to more than one window.\n",
    "\n",
    "| Event Timestamp | Arrival Time | Lag (min) | Order Count | Windows Covering This Event    | Late Arrival |\n",
    "|-----------|--------------|-----------|-------------|--------------------------------------|--------------|\n",
    "| 12:01 PM  | 12:01 PM     | 0         | 5           | 12:00 - 12:10                        |              |\n",
    "| 12:06 PM  | 12:06 PM     | 0         | 8           | 12:00 - 12:10, 12:05 - 12:15         |              |\n",
    "| 12:07 PM  | 12:12 PM     | 5         | 2           | 12:00 - 12:10, 12:05 - 12:15         | ‚úÖ           |\n",
    "| 12:07 PM  | 12:26 PM     | 9         | 1           | 12:00 - 12:10, 12:05 - 12:15         | ‚ùå           |\n",
    "| 12:12 PM  | 12:12 PM     | 0         | 10          | 12:05 - 12:15, 12:10 - 12:20         |              |\n",
    "| 12:13 PM  | 12:18 PM     | 5         | 4           | 12:05 - 12:15, 12:10 - 12:20         | ‚úÖ           |\n",
    "| 12:13 PM  | 12:40 PM     | 27        | 1           | 12:05 - 12:15, 12:10 - 12:20         | ‚ùå           |\n",
    "| 12:17 PM  | 12:17 PM     | 0         | 7           | 12:10 - 12:20, 12:15 - 12:25         |              |\n",
    "\n",
    "    \n",
    "| Window Start | Window End | Orders Before Watermark | Orders Arriving Late (Before 5 min) | Orders Arriving Too Late (Ignored) | Final Count |\n",
    "|--------------|------------|-------------------------|--------------------------------------|------------------------------------|-------------|\n",
    "| 12:00 PM     | 12:10 PM   | 5 + 8 = 13              | +2 at 12:12 PM                       | +1 at 12:26 PM                     | 13 + 2 = 15 |\n",
    "| 12:05 PM     | 12:15 PM   | 8 + 10 = 18             | +2 at 12:12 PM, +4 at 12:18 PM       | +1 at 12:26 PM, +1 at 12:40 PM     | 18 + 2 + 4 = 24 |\n",
    "| 12:10 PM     | 12:20 PM   | 10 + 7 = 17             | +4 at 12:18 PM                       | +1 at 12:40 PM                     | 17 + 4 = 21 |\n",
    "| 12:15 PM     | 12:25 PM   | 7                       |                                      |                                    | 7           |\n",
    "\n",
    "2. **Tumbling Window:**:\n",
    "It groups data into fixed-size intervals without overlap. Every event belongs to exactly one window.\n",
    "   - A new window starts every 10 minutes.\n",
    "   - Each window processes only the data within its interval.\n",
    "   - Non-overlapping ‚Üí Each event belongs to just one window.\n",
    "\n",
    "| Event Timestamp | Arrival Time | Lag (min) | Order Count | Window Start | Window End | Late Arrival |\n",
    "|-----------|--------------|-----------|-------------|--------------|------------|--------------|\n",
    "| 12:01 PM  | 12:01 PM     | 0         | 5           | 12:00 PM     | 12:10 PM   |              |\n",
    "| 12:05 PM  | 12:05 PM     | 0         | 8           | 12:00 PM     | 12:10 PM   |              |\n",
    "| 12:08 PM  | 12:13 PM     | 5         | 2           | 12:00 PM     | 12:10 PM   | ‚úÖ            |\n",
    "| 12:09 PM  | 12:15 PM     | 6         | 1           | 12:00 PM     | 12:10 PM   | ‚ùå            |\n",
    "| 12:12 PM  | 12:12 PM     | 0         | 10          | 12:10 PM     | 12:20 PM   |              |\n",
    "| 12:17 PM  | 12:17 PM     | 0         | 7           | 12:10 PM     | 12:20 PM   |              |\n",
    "| 12:18 PM  | 12:22 PM     | 4         | 3           | 12:10 PM     | 12:20 PM   | ‚úÖ            |\n",
    "| 12:19 PM  | 12:26 PM     | 7         | 2           | 12:10 PM     | 12:20 PM   | ‚ùå            |\n",
    "\n",
    "\n",
    "| Window Start | Window End | Orders Before Watermark | Orders Arriving Late (Before 5 min) | Orders Arriving Too Late (Ignored) | Final Count |\n",
    "|--------------|------------|-------------------------|--------------------------------------|------------------------------------|-------------|\n",
    "| 12:00 PM     | 12:10 PM   | 5 + 8 = 13              | +2 at 12:12 PM ‚Üí accepted            | +1 at 12:26 PM           | 13 + 2 = 15 |\n",
    "| 12:10 PM     | 12:20 PM   | 10 + 7 = 17             | +3 at 12:22 PM ‚Üí accepted            | +2 at 12:36 PM           | 17 + 3 = 20 |\n",
    "\n",
    "\n",
    "3. **Watermark:**:\n",
    "A Watermark defines how late data can be before Spark discards it from processing.\n",
    "   - A mechanism to handle late data.\n",
    "   - Specifies the maximum delay allowed for late data.\n",
    "   - Helps in cleaning up old state and reducing memory usage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67bdeddb-4ef9-4567-a345-b5bed5a5669e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### *Tumbling Window with Watermark - Needs Update\n",
    "In this section, we implement a tumbling window aggregation in PySpark Structured Streaming to count the number of orders per warehouse over a 10-second window. The results are written to a Gold Table in Delta format for further analysis.\n",
    "\n",
    "**Read Stream**\n",
    "- `silver_stream` ‚Üí Represents a streaming DataFrame containing incoming order data.\n",
    "- `withWatermark(\"event_timestamp\", \"5 seconds\")` ‚Üí Handles late data, allowing events to arrive up to 5 seconds late before discarding them.\n",
    "- `groupBy(window(col(\"event_timestamp\"), \"10 seconds\"), col(\"warehouse_location\"))` ‚Üí Groups data into non-overlapping 10-second tumbling windows, while also grouping by warehouse_location.\n",
    "- `agg(count(\"order_id\").alias(\"total_orders\"))` ‚Üí Counts the number of orders (order_id) per warehouse in each window.\n",
    "\n",
    "**Write Stream**\n",
    "- `.writeStream.format(\"delta\")` ‚Üí Saves results in Delta format, which supports ACID transactions and efficient updates.\n",
    "- `.outputMode(\"append\")` ‚Üí Writes new aggregated data incrementally, ensuring continuous updates to the Gold Table.\n",
    "- `.option(\"checkpointLocation\", \"/Volumes/green/gold/files/checkpoints_tumbling/\")` ‚Üí Uses a checkpoint directory to track progress and prevent data loss.\n",
    "- `.table(\"green.gold.orders_tumbling\")` ‚Üí Stores the results in the Gold Layer of the data lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "122ece6d-4ef1-4700-a851-3d1413309c92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, count, col\n",
    "\n",
    "tumbling_window_stream = (silver_stream\n",
    "    .withWatermark(\"event_timestamp\", \"20 seconds\") \n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"10 seconds\"),\n",
    "        col(\"warehouse_location\")\n",
    "    )\n",
    "    .agg(count(\"order_id\").alias(\"total_orders\"), avg(\"quantity\").alias(\"avg_quantity\"))\n",
    ")\n",
    "\n",
    "# Writing the result to the Gold Table\n",
    "tumbling_window_query = (tumbling_window_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")  # \"append\" allows incremental processing\n",
    "    .option(\"checkpointLocation\", \"/Volumes/green/gold/files/checkpoints/orders_tumbling\")\n",
    "    .table(\"green.gold.orders_tumbling\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f0fd9e9-8fca-4f41-b9e8-a27e9217fac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### *Sliding Window with Watermark - Needs Update\n",
    "This section demonstrates how to perform a sliding window aggregation in PySpark Structured Streaming to calculate the average quantity of orders for each product over time. The results are continuously written to a Gold Table in Delta format for further analysis.\n",
    "\n",
    "**Read Stream**\n",
    "- `silver_stream` ‚Üí Represents a streaming DataFrame containing incoming order data.\n",
    "- `withWatermark(\"event_timestamp\", \"20 minutes\")` ‚Üí Allows Spark to wait for late data up to 20 minutes before dropping it.\n",
    "- `groupBy(window(col(\"event_timestamp\"), \"10 minutes\", \"5 minutes\"), col(\"product_id\"))` ‚Üí Creates a sliding window of 10 minutes, updated every 5 minutes.\n",
    "  - Each event contributes to two overlapping windows: one covering the past 10 minutes, and another overlapping 5 minutes later.\n",
    "  - This ensures more frequent updates and smooth trend tracking.\n",
    "- `agg(avg(\"quantity\").alias(\"avg_quantity\"))` ‚Üí Computes the average quantity ordered per product within each window.\n",
    "\n",
    "**Write Stream**\n",
    "- `.writeStream.format(\"delta\")` ‚Üí Saves results in Delta format, ensuring efficient incremental updates and ACID compliance.\n",
    "- `.outputMode(\"append\")` ‚Üí Ensures that only new aggregated results are added to the Gold Table.\n",
    "- `.option(\"checkpointLocation\", \"/Volumes/green/gold/files/checkpoints_sliding/\")` ‚Üí Uses a checkpoint directory for tracking progress and maintaining fault tolerance.\n",
    "- `.table(\"green.gold.orders_sliding\")` ‚Üí Stores the results in the Gold Layer of the data lakehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d411406-48df-4082-b127-c7f6c865ed08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "sliding_window_stream = (silver_stream\n",
    "    .withWatermark(\"event_timestamp\", \"20 seconds\")\n",
    "    .groupBy(\n",
    "        window(col(\"event_timestamp\"), \"10 seconds\", \"5 seconds\"),\n",
    "        col(\"warehouse_location\")\n",
    "    )\n",
    "    .agg(count(\"order_id\").alias(\"total_orders\"), avg(\"quantity\").alias(\"avg_quantity\"))\n",
    ")\n",
    "\n",
    "# Writing the result to the Gold Table\n",
    "sliding_window_query = (sliding_window_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")  # \"append\" mode processes only new updates\n",
    "    .option(\"checkpointLocation\", \"/Volumes/green/gold/files/checkpoints/orders_sliding\")\n",
    "    .table(\"green.gold.orders_sliding\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f982317a-78c1-4e02-ad45-72dabbb22ba7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Create Kafka Order Producer Simulation\n",
    "---\n",
    "\n",
    "This section sets up a **Kafka Producer** to simulate and stream **real-time supply chain order data** into a Kafka topic. This enables the **ingestion of synthetic order events**, which will be processed downstream in the **Green-Blue Medallion architecture**.\n",
    "\n",
    "#### **Key Steps:**\n",
    "\n",
    "1. **Kafka Configuration:**  \n",
    "   - Defines the Kafka **bootstrap servers** for message transmission.  \n",
    "   - Specifies the **Kafka topic (`supplychain-orders`)** where messages will be published.  \n",
    "\n",
    "2. **Kafka Producer Setup:**  \n",
    "   - Initializes a **KafkaProducer** with:  \n",
    "     - **UTF-8 key serialization** for message keys.  \n",
    "     - **JSON serialization** for the message values.  \n",
    "\n",
    "3. **Generating Supply Chain Order Messages:**  \n",
    "   - Defines a function to **simulate order events**, including:  \n",
    "     - `order_id`: Unique order identifier.  \n",
    "     - `product_id`: Random product ID.  \n",
    "     - `quantity`: Random order quantity.  \n",
    "     - `warehouse_location`: Random warehouse identifier.  \n",
    "     - `status`: Random order status (`shipped`, `pending`, `delivered`, `cancelled`).  \n",
    "     - `order_timestamp`: Timestamp of the order in **ISO 8601 UTC format**.  \n",
    "\n",
    "4. **Streaming Data into Kafka:**  \n",
    "   - Sends **10 simulated order messages** to Kafka with a **5-second delay** between each.  \n",
    "   - Uses the `order_id` as the message key for partitioning.  \n",
    "   - Prints confirmation logs for tracking messages.  \n",
    "\n",
    "5. **Finalizing the Producer:**  \n",
    "   - **Flushes** the Kafka buffer to ensure all messages are sent, guaranteeing that no data is lost before closing the producer.\n",
    "   - **Closes** the producer to free resources.  \n",
    "\n",
    "This Kafka producer **emulates real-world order processing events**, providing a **continuous data stream** that feeds into **Databricks‚Äô Medallion architecture**, where it will be further processed and stored in the **bronze layer**. üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7b92fa-80b3-4423-bc00-1b3426ba2710",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate and Send Supply Chain Messages to Kafka"
    }
   },
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Setting up the Kafka Producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=kafka_bootstrap_servers,\n",
    "    key_serializer=lambda k: k.encode('utf-8') if k else None,\n",
    "    value_serializer=lambda value: json.dumps(value).encode(\"utf-8\"),\n",
    ")\n",
    "\n",
    "# Function to generate supply chain messages\n",
    "def generate_supply_chain_message(order_id):\n",
    "    return {\n",
    "        \"order_id\": order_id,\n",
    "        \"product_id\": f\"P-{random.randint(1000, 1005)}\",\n",
    "        \"quantity\": random.randint(1, 100),\n",
    "        \"warehouse_location\": f\"WH-{random.randint(1, 3)}\",\n",
    "        \"status\": random.choice([\"shipped\", \"pending\", \"delivered\", \"cancelled\"]),\n",
    "        \"event_timestamp\": datetime.now(timezone.utc).isoformat()\n",
    "    }\n",
    "\n",
    "# Sending supply chain messages to Kafka\n",
    "for i in range(20):\n",
    "    key = str(i)\n",
    "    message = generate_supply_chain_message(i)\n",
    "    producer.send(kafka_topic, key=key, value=message)\n",
    "    print(f\"Sent: {message} with key: {key}\")\n",
    "    time.sleep(random.randint(1, 15))\n",
    "\n",
    "producer.flush()\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84f0db9-752b-46ea-8bc8-e51384312a10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming Table Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290a0073-623e-49cb-8843-528936dad5d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream Bronze Orders"
    }
   },
   "outputs": [],
   "source": [
    "# Display the streaming tables prior to sending messages\n",
    "spark.readStream.table(\"green.bronze.orders\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7f575e9-6f2b-4a87-a364-79189e84acb4",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1741542457895}",
       "tableResultIndex": 0
      }
     },
     "title": "Stream Silver Orders"
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.table(\"green.silver.orders\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5502f7b0-eb04-485b-9c64-86e7ade0e161",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream Gold Orders Tumbling"
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.table(\"green.gold.orders_tumbling\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64debc73-fca5-41e3-aeb2-0e104f0d67da",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stream Gold Orders Sliding"
    }
   },
   "outputs": [],
   "source": [
    "spark.readStream.table(\"green.gold.orders_sliding\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82672d00-73b2-430e-9ce0-ac8f696457a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Clear Tables and Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c92311a-22dc-4956-bd9f-198a91459766",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Clear streaming tables and checkpoints\n",
    "tables = [\"green.bronze.orders\", \"green.silver.orders\", \"green.gold.orders\", \"green.gold.orders_tumbling\", \"green.gold.orders_sliding\"]\n",
    "for table in tables:\n",
    "    truncate_query = f\"DROP TABLE {table}\"\n",
    "    try: spark.sql(truncate_query)\n",
    "    except: pass\n",
    "\n",
    "    table_split = table.split(\".\")\n",
    "    dbutils.fs.rm(f\"/Volumes/{table_split[0]}/{table_split[1]}/files/checkpoints/\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74407dcb-1799-4116-8c13-d4a8ecaab7a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for s in spark.streams.active:\n",
    "    print(s.name)\n",
    "    s.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9026a38d-aefd-43b8-b59e-dba3832c6d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2488146137546457,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "t2a-databricks-msk-medallion-pyspark-streaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
