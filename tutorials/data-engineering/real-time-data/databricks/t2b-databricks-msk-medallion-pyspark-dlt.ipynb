{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4228cd-44e5-48ae-9012-6aacec8fc206",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import from_json, col, avg, count, window, expr, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9c73748-c5c0-4cd1-91fc-4cb87d55e778",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# In production use a secret for the bootstrap_servers, this is just for demo purposes and it is deleted after each demo\n",
    "KAFKA_BOOTSTRAP_SERVERS = \"b-1.msksilveraiwolfuseast.nqmaxv.c11.kafka.us-east-1.amazonaws.com:9092,b-2.msksilveraiwolfuseast.nqmaxv.c11.kafka.us-east-1.amazonaws.com:9092\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "63001bb8-4a3c-464d-bc43-7ac86085d00a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Multiplex Bronze\n",
    "\n",
    "A Multiplex Bronze Streaming Table is a data table in a lakehouse architecture designed to ingest and store raw streaming data from multiple sources into a single, unified table, rather than maintaining separate bronze tables for each topic. This approach is commonly used in Databricks and Delta Lake architectures, particularly within the medallion architecture, which organizes data into three layers:\n",
    "\n",
    "- Bronze Layer (Raw Data) – Stores unprocessed, raw data from various sources, including real-time streams.\n",
    "- Silver Layer (Cleansed Data) – Data is cleaned, deduplicated, and formatted for structured analytics.\n",
    "- Gold Layer (Aggregated Data) – Optimized for business intelligence and reporting.\n",
    "\n",
    "**Breaking Down \"Multiplex Bronze Streaming Table\"**\n",
    "- Multiplex: Handles multiple streaming topics (e.g., Kafka topics, event hubs) within the same table.\n",
    "- Bronze: Represents the raw ingestion layer where all data lands before transformation.\n",
    "- Streaming Table: Supports continuous data ingestion from real-time sources.\n",
    "\n",
    "A Multiplex Bronze Table is beneficial in scenarios where multiple event streams or sources are ingested at scale. Here’s why:\n",
    "\n",
    "✅ Simplified Data Ingestion\n",
    "\n",
    "Instead of managing separate ingestion pipelines for each streaming topic, a single pipeline ingests all raw data into one table.\n",
    "Reduces operational overhead in pipeline development and maintenance.\n",
    "\n",
    "\n",
    "✅ Better Query Performance for Downstream Consumers\n",
    "\n",
    "When Silver and Gold tables need to process data from multiple sources, a single source of truth (multiplex table) simplifies joins and aggregations.\n",
    "Avoids the need for complex union queries across multiple bronze tables.\n",
    "\n",
    "✅ Improved Governance and Observability\n",
    "\n",
    "Centralized auditing and logging for all ingested data.\n",
    "Easier to apply access control policies, monitoring, and data quality checks in one place.\n",
    "\n",
    "✅ Faster Processing & Streaming Performance\n",
    "\n",
    "With Delta Lake's metadata handling and compaction features, a single partitioned table (e.g., partitioned by event_type) provides faster lookups than multiple small tables.\n",
    "Reduces the risk of small file problems that arise with many separate topic-based tables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ba8a63d-7072-451a-811c-e8f0befbf91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_kafka_stream(topic: str):\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\n",
    "        .option(\"subscribe\", topic)\n",
    "        .option(\"startingOffsets\", \"earliest\")\n",
    "        .load()\n",
    "        .selectExpr(\"CAST(value AS STRING) AS parsed_value\", \"CAST(key AS STRING) AS parsed_key\", \"*\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"multipex_bronze\",\n",
    "    comment=\"Bronze table for streaming ingestion\",\n",
    "    table_properties={\n",
    "        \"pipelines.reset.allowed\": \"false\",\n",
    "        \"quality\": \"bronze\"\n",
    "    }\n",
    ")\n",
    "def multipex_bronze():\n",
    "    topics = [\n",
    "        \"orders-raw\", \n",
    "        \"shipment-contents\", \n",
    "        \"inventory-updates\", \n",
    "        \"shipment-status\", \n",
    "        \"customer-feedback\"\n",
    "    ]\n",
    "    \n",
    "    # Read all topics and union them into a single dataframe\n",
    "    streams = [read_kafka_stream(topic).withColumn(\"topic\", expr(f\"'{topic}'\")) for topic in topics]\n",
    "    return reduce(lambda df1, df2: df1.union(df2), streams)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c96b62b-f427-474f-ab59-176addbd828e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver\n",
    "\n",
    "A Silver Table in a data lakehouse architecture refers to a cleansed and enriched dataset that sits between the raw Bronze layer and the curated Gold layer. It is part of the medallion architecture used in Databricks, Delta Lake, and modern data platforms.\n",
    "\n",
    "Medallion Architecture Overview\n",
    "- Bronze Tables (Raw Data) – Stores raw, unprocessed data from various sources.\n",
    "- Silver Tables (Cleansed Data) – Processes, cleans, and deduplicates data from Bronze before making it available for analytics.\n",
    "- Gold Tables (Aggregated Data) – Optimized for business intelligence (BI), reporting, and machine learning.\n",
    "\n",
    "**What Makes a Silver Table Different?**\n",
    "\n",
    "✅ Cleansed & Preprocessed Data\n",
    "\n",
    "Removes duplicates, corrupt records, and null values.\n",
    "Standardizes formats (e.g., date/time, data types).\n",
    "\n",
    "✅ Joins & Enrichments\n",
    "\n",
    "Combines data from multiple Bronze tables.\n",
    "Adds reference data, metadata, or calculated columns.\n",
    "\n",
    "✅ Optimized for Query Performance\n",
    "\n",
    "Uses Delta Lake optimizations (Z-ordering, indexing, caching).\n",
    "Reduces unnecessary scans by partitioning data.\n",
    "\n",
    "✅ Supports Incremental Loads\n",
    "\n",
    "Uses Change Data Capture (CDC) or merge operations to update records efficiently.\n",
    "Handles slowly changing dimensions (SCDs) in a structured way.\n",
    "\n",
    "✅ Prepares Data for Gold Layer\n",
    "\n",
    "Feeds aggregated, business-ready datasets into Gold tables for analytics and BI tools.\n",
    "\n",
    "**Why Use Silver Tables?**\n",
    "- Improves Data Quality – Ensures clean, trustworthy data before analytics.\n",
    "- Enhances Performance – Queries run faster compared to raw Bronze data.\n",
    "- Reduces Complexity in Gold Tables – Keeps business-layer datasets optimized and structured.\n",
    "- Supports Machine Learning & AI – ML models require structured, well-processed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5910bbbc-cc5e-4468-9dc2-147ee9ff914b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Schemas & Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28dcf56c-6089-452c-9b6d-5e7eaba5d36b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Define Schemas for Orders and Shipments Data"
    }
   },
   "outputs": [],
   "source": [
    "# Define schema structures\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "shipment_content_schema = StructType([\n",
    "    StructField(\"shipment_id\", StringType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"quantity\", IntegerType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "inventory_schema = StructType([\n",
    "    StructField(\"warehouse_location\", StringType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"stock_level\", IntegerType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "shipment_status_schema = StructType([\n",
    "    StructField(\"shipment_id\", StringType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"current_status\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])\n",
    "\n",
    "customer_feedback_schema = StructType([\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"rating\", IntegerType()),\n",
    "    StructField(\"comment\", StringType()),\n",
    "    StructField(\"event_timestamp\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65b94258-3041-4fe4-bd02-baacd4b0e68c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Stream Parsing Function and Order Validation Rules"
    }
   },
   "outputs": [],
   "source": [
    "# Define streaming tables\n",
    "def parse_stream(schema, topic):\n",
    "    return (\n",
    "        dlt.readStream(\"multipex_bronze\")\n",
    "        .filter(col(\"topic\") == topic)\n",
    "        .select(from_json(col(\"parsed_value\"), schema).alias(\"json_value\"), \"parsed_key\")\n",
    "        .selectExpr(\"parsed_key AS key\", \"json_value.*\")\n",
    "    )\n",
    "\n",
    "def parse_read(schema, topic):\n",
    "    return (\n",
    "        spark.read.table(\"multipex_bronze\")\n",
    "        .filter(f\"topic = '{topic}'\")\n",
    "        .select(from_json(col(\"parsed_value\"), schema).alias(\"json_value\"), \"parsed_key\")\n",
    "        .selectExpr(\"parsed_key AS key\", \"json_value.*\")\n",
    "    )\n",
    "\n",
    "order_rules = {\n",
    "    \"invalid_qty\": \"quantity != 0\"\n",
    "}\n",
    "\n",
    "quarentine_rules = {}\n",
    "quarentine_rules[\"invalid_records\"] = f\"NOT({' AND '.join(order_rules.values())})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d90ae2-91ef-490b-8e36-62887b8c3e15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Definition and Data Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7d94a4-27b8-463f-b197-24ca36a792e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Define Tables for Aggregated and Cancelled Orders"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"orders_silver\",\n",
    "    comment=\"Aggregated orders partition\"\n",
    ")\n",
    "@dlt.expect_all_or_drop(order_rules)\n",
    "def orders_silver():\n",
    "    return parse_stream(orders_schema, \"orders-raw\")\n",
    "\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"orders_quarentine_silver\",\n",
    "    comment=\"Aggregated orders partition that have been cancelled\"\n",
    ")\n",
    "@dlt.expect_all_or_drop(quarentine_rules)\n",
    "def orders_quarentine_silver():\n",
    "    return parse_stream(orders_schema, \"orders-raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c15ed310-dbc5-4d7c-ae73-4d0ffa686968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Change Data Capture\n",
    "---\n",
    "\n",
    "Change Data Capture (CDC) is a technique used to track and capture changes (INSERTS, UPDATES, and DELETES) in a source dataset and apply them efficiently to a target dataset. In the context of Delta Live Tables (DLT), CDC helps in managing incremental data updates in a declarative, scalable, and automated way.\n",
    "\n",
    "DLT is a framework built on Apache Spark and Delta Lake that simplifies building ETL (Extract, Transform, Load) pipelines. It provides built-in support for CDC by enabling you to capture changes from streaming or batch sources and apply them efficiently to your target Delta table.\n",
    "\n",
    "See the official documentation here: https://docs.databricks.com/aws/en/dlt/cdc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99b17b9e-2c51-4d8e-b234-fa0f320dbf07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Change Data Capture Materialized View Example\n",
    "\n",
    "This materialized view mimics the behavior of CDC by deduplicating records using a window function and enforcing a constraint to keep only the latest record per shipment_id and product_id.\n",
    "It allows you to preview the transformations that apply_changes() would apply before using it in a live CDC pipeline.\n",
    "\n",
    "> This is not a best practice way of doing CDC but it shows the idea of deduplicating and applying the latest changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da622a49-1981-4e6b-90c1-76d179b03155",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Create Shipment Content Table with Row Numbers"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"shipment_content_silver\",\n",
    "    comment=\"A materialized view to show what the apply_changes() will do\"\n",
    ")\n",
    "@dlt.expect_all_or_drop({\"duplicated_records\": \"row_num = 1\"})\n",
    "def shipment_content_silver():\n",
    "    df = parse_read(shipment_content_schema, \"shipment-contents\")\n",
    "\n",
    "    # Add a row number to the dataframe to identify the latest record\n",
    "    window_spec = Window.partitionBy(\"shipment_id\", \"product_id\").orderBy(col(\"event_timestamp\").desc())\n",
    "    df_with_row_num = df.withColumn(\"row_num\", row_number().over(window_spec))\n",
    "\n",
    "    return df_with_row_num\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3e840bd-b54f-4c0b-943c-cff333aae9f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Delta Live Table CDC Slow Changing Dimension\n",
    "\n",
    "Slowly Changing Dimensions (SCD) refer to a technique for managing historical changes in a dataset, particularly for dimensional tables in a data warehouse. In Delta Live Tables (DLT), SCDs can be managed using the APPLY CHANGES feature.\n",
    "\n",
    "There are two commonly used SCD types in Delta Live Tables:\n",
    "\n",
    "1. **SCD Type 1 (Overwrite)**\n",
    "   - SCD Type 1 does not retain history.\n",
    "   - When a change occurs (e.g., an update in the source data), the existing record is overwritten.\n",
    "   - This approach ensures only the latest data is stored.\n",
    "   - **Use Case:**\n",
    "     - Used when historical tracking is not required.\n",
    "     - Best for scenarios where corrections or updates must replace old data.\n",
    "     - Example: Correcting customer contact details (phone number, email).\n",
    "\n",
    "2. **SCD Type 2 (Historical)**\n",
    "   - SCD Type 2 preserves historical records.\n",
    "   - When a change occurs, a new row is inserted with a different start_date and end_date, while the previous record is marked as inactive.\n",
    "   - This allows tracking historical changes over time.\n",
    "   - **Use Case:**\n",
    "     - Used when maintaining a history of changes is necessary.\n",
    "     - Best for tracking customer address changes, product price changes, employee role history, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3fb759b-f9d0-4242-ba00-b98529ed3ee6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Stream Parsing for Shipment Content View"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view(\n",
    "    name=\"shipment_content_source_silver\",\n",
    "    comment=\"Source table for the apply_changes() function\"\n",
    ")\n",
    "def shipment_content_source_silver():\n",
    "    return parse_stream(shipment_content_schema, \"shipment-contents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea54ed50-3ed0-4e3d-b142-5ae9add993e8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Type 1 SCD Handling"
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\n",
    "    name=\"shipment_content_scd1_silver\",\n",
    "    comment=\"Slowly changing dimensional table with updates\",\n",
    ")\n",
    "dlt.apply_changes(\n",
    "    target = \"shipment_content_scd1_silver\",\n",
    "    source = \"shipment_content_source_silver\",\n",
    "    keys = [\"shipment_id\", \"product_id\"],\n",
    "    sequence_by = col(\"event_timestamp\"),\n",
    "    stored_as_scd_type = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f7511c2-dc1c-40db-82bc-cbc4070f6fc6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Type 2 SCD Handling"
    }
   },
   "outputs": [],
   "source": [
    "dlt.create_streaming_table(\n",
    "    name=\"shipment_content_scd2_silver\",\n",
    "    comment=\"Slowly changing dimensional table without updates\",\n",
    ")\n",
    "dlt.apply_changes(\n",
    "    target = \"shipment_content_scd2_silver\",\n",
    "    source = \"shipment_content_source_silver\",\n",
    "    keys = [\"shipment_id\", \"product_id\"],\n",
    "    sequence_by = col(\"event_timestamp\"),\n",
    "    stored_as_scd_type = 2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c415201b-ae93-4257-a7b6-b9011a5d03b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Applying SCD Type 1 to the Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c35e2dc-eb8c-4089-b610-3fa008fef580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inventory \n",
    "@dlt.view(\n",
    "    name=\"inventory_source_silver\",\n",
    "    comment=\"Source view for the apply_changes function\"\n",
    ")\n",
    "def inventory_source_silver():\n",
    "    return parse_stream(inventory_schema, \"inventory-updates\")\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name=\"inventory_scd1_silver\",\n",
    "    comment=\"Slowly changing dimensional table with updates\",\n",
    ")\n",
    "dlt.apply_changes(\n",
    "    target = \"inventory_scd1_silver\",\n",
    "    source = \"inventory_source_silver\",\n",
    "    keys = [\"warehouse_location\", \"product_id\"],\n",
    "    sequence_by = col(\"event_timestamp\"),\n",
    "    stored_as_scd_type = 1\n",
    ")\n",
    "    \n",
    "# Shipment Status\n",
    "@dlt.view(\n",
    "    name=\"shipment_status_source_silver\",\n",
    "    comment=\"Source view for the apply_changes function\"\n",
    ")\n",
    "def shipment_status_silver():\n",
    "    return parse_stream(shipment_status_schema, \"shipment-status\")\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name=\"shipment_status_scd1_silver\",\n",
    "    comment=\"Slowly changing dimensional table with updates\",\n",
    ")\n",
    "dlt.apply_changes(\n",
    "    target = \"shipment_status_scd1_silver\",\n",
    "    source = \"shipment_status_source_silver\",\n",
    "    keys = [\"shipment_id\", \"order_id\"],\n",
    "    sequence_by = col(\"event_timestamp\"),\n",
    "    stored_as_scd_type = 1\n",
    ")\n",
    "\n",
    "\n",
    "# Customer Feedback\n",
    "@dlt.view(\n",
    "    name=\"customer_feedback_source_silver\",\n",
    "    comment=\"Source view for the apply_changes function\"\n",
    ")\n",
    "def customer_feedback_silver():\n",
    "    return parse_stream(customer_feedback_schema, \"customer-feedback\")\n",
    "\n",
    "dlt.create_streaming_table(\n",
    "    name=\"customer_feedback_scd1_silver\",\n",
    "    comment=\"Slowly changing dimensional table with updates\",\n",
    ")\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_feedback_scd1_silver\",\n",
    "    source = \"customer_feedback_source_silver\",\n",
    "    keys = [\"order_id\", \"customer_id\"],\n",
    "    sequence_by = col(\"event_timestamp\"),\n",
    "    stored_as_scd_type = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa689afc-322d-4fdb-b0d1-e247e182cd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Gold\n",
    "---\n",
    "\n",
    "Gold tables represent the final, refined, and business-ready layer of the data pipeline. They contain aggregated, enriched, and highly optimized data, ready for business intelligence (BI), analytics, and reporting.\n",
    "\n",
    "**Purpose of Gold Tables**\n",
    "1. Business-Ready Analytics\n",
    "Gold tables store cleaned and transformed data tailored for business decision-making.\n",
    "They provide aggregated KPIs, trends, and metrics that can be used directly by analysts and executives.\n",
    "\n",
    "2. Performance Optimization for Reporting & Queries\n",
    "Since Gold tables store pre-aggregated and denormalized data, they reduce the need for heavy computations in real-time queries.\n",
    "This improves query performance for BI tools like Power BI, Tableau, or Looker.\n",
    "\n",
    "3. Simplification of Data Access\n",
    "Gold tables make it easy for non-technical users (e.g., business analysts) to access data without complex SQL queries.\n",
    "Instead of navigating multiple Silver tables, users get one source of truth.\n",
    "\n",
    "4. Reducing Compute Costs\n",
    "Since complex transformations (joins, aggregations) are precomputed in Gold tables, the overall query cost is reduced.\n",
    "This is critical in big data environments like Databricks where compute costs can be high.\n",
    "\n",
    "5. Enabling Machine Learning & AI\n",
    "Gold tables often serve as the feature store for machine learning models.\n",
    "They provide cleaned, aggregated, and structured datasets that are ready for model training.\n",
    "\n",
    "Gold tables bridge raw data with actionable insights, making data easily consumable for business, BI, and machine learning. They improve performance, simplify data access, and reduce costs, making them essential in modern data pipelines. 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc784d3e-64c3-48e2-b469-c731cf25df6d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Combine Orders and Silver Tables for Final Output"
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name=\"wide_orders_gold\",\n",
    "    comment=\"Final wide table combining all silver tables\"\n",
    ")\n",
    "@dlt.expect_all_or_drop({\"no_shipment\": \"shipment_id IS NOT NULL\"})\n",
    "def wide_orders_gold():\n",
    "    orders = dlt.read(\"orders_silver\")\n",
    "    shipments = (dlt.read(\"shipment_content_scd1_silver\")\n",
    "        .drop(\"key\", \"event_timestamp\")\n",
    "        .withColumnRenamed(\"quantity\", \"shipped_quantity\")\n",
    "    )\n",
    "    inventory = (dlt.read(\"inventory_scd1_silver\")\n",
    "        .drop(\"key\", \"event_timestamp\")             \n",
    "    )\n",
    "    shipment_status = (dlt.read(\"shipment_status_scd1_silver\")\n",
    "        .drop(\"key\", \"event_timestamp\")\n",
    "    )\n",
    "    feedback = (dlt.read(\"customer_feedback_scd1_silver\")\n",
    "        .drop(\"key\", \"event_timestamp\")\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        orders\n",
    "        .join(shipments, [\"order_id\", \"product_id\"], \"left\")\n",
    "        .join(inventory, \"product_id\", \"left\")\n",
    "        .join(shipment_status, [\"order_id\", \"shipment_id\"], \"left\")\n",
    "        .join(feedback, \"order_id\", \"left\")\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "t2b-databricks-msk-medallion-pyspark-dlt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
