{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0c2314-e696-4146-b353-a4c9ca718e93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Introduction\n",
    "---\n",
    "    \n",
    "In this tutorial, we explore and compare two optimization strategies in Databricks: Partitioning with Z-Ordering and Liquid Clustering. These techniques help optimize data skipping and read performance for large Delta Lake tables.\n",
    "\n",
    "**Explanation**\n",
    "- Partitioning splits data into directories based on column values.\n",
    "- Z-Ordering reorders data within files to colocate related information.\n",
    "- Liquid Clustering introduces automatic optimization without strict partition boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70fded0b-6be4-41b3-97e3-70c65a1452de",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Imports & Session"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import expr\n",
    "from pprint import pprint\n",
    "import json \n",
    "\n",
    "# spark comes instatiated out of the box in databricks. However, if you're running locally, you'll need to instantiate it.\n",
    "# We will instantiate to showcase how to instantiate it.\n",
    "spark = SparkSession.builder.appName(\"DeltaOptimization\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "605b4e28-4698-42db-bc22-674c44823b47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Dataset Preparation\n",
    "---\n",
    "We'll create a synthetic dataset that simulates a typical claims dataset, saved as a Delta table.\n",
    "\n",
    "**Explanation**\n",
    "- The dataset includes `claim_id`, `member_id`, `claim_date`, `claim_amount`, and `state`.\n",
    "- We'll use this dataset to demonstrate both optimization approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950caeb4-3c7b-4e42-ab71-3fcdbc812495",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Dataset Mockup"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.range(0, 100)\n",
    "claims_df = df.withColumn(\"claim_id\", expr(\"id\")) \\\n",
    "              .withColumn(\"member_id\", expr(\"id % 100000\")) \\\n",
    "              .withColumn(\"claim_date\", expr(\"date_add('2020-01-01', cast(id % 10 as int))\")) \\\n",
    "              .withColumn(\"state\", expr(\"CASE WHEN id % 5 = 0 THEN 'CA' WHEN id % 5 = 1 THEN 'TX' WHEN id % 5 = 2 THEN 'NY' WHEN id % 5 = 3 THEN 'FL' ELSE 'WA' END\")) \\\n",
    "              .withColumn(\"claim_amount\", expr(\"round(rand() * 1000, 2)\"))\n",
    "claims_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d528bc8-69d0-4f92-ac1c-a9dd343db04e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#TODO: Add ingestion time clustering here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f454c8fc-295f-4172-a1e4-52e8c015d725",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3. Traditional Partitioning + Z-Ordering\n",
    "---\n",
    "We write the data into a Delta table using `state` as the partition column and then apply Z-Ordering on `claim_date`.\n",
    "\n",
    "**Explanation**\n",
    "- Partitioning creates directory structures like `/state=CA/`.\n",
    "- Z-Ordering within each partition sorts data to improve skipping on `claim_date`.\n",
    "- Z-Ordering physically colocates similar `claim_date` values into the same files, so when queries filter by date, Spark can skip unrelated files more effectively.\n",
    "- The Spark optimizer uses **min/max statistics per file** to determine if a file can be skipped for a query predicate.\n",
    "\n",
    "**File Structure Example**:\n",
    "```\n",
    "/tmp/delta/claims_partitioned/\n",
    "├── state=CA/\n",
    "│   ├── part-0000.snappy.parquet\n",
    "│   └── ...\n",
    "├── state=TX/\n",
    "│   ├── part-0000.snappy.parquet\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b344bbb5-e9f1-462e-80a4-35b5ccecca0d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Removing old data"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the old data for new tutorial runs\n",
    "%rm -rf /dbfs/tmp/delta/claims_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa458359-084f-46ff-b446-123d34229788",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Creating Partitioned Data"
    }
   },
   "outputs": [],
   "source": [
    "claims_df.write.format(\"delta\") \\\n",
    "    .partitionBy(\"state\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/delta/claims_partitioned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7eeafa-c6bf-4c59-8e4d-f1f1e18933a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Observing Partitions"
    }
   },
   "outputs": [],
   "source": [
    "# Partition structure\n",
    "%ls /dbfs/tmp/delta/claims_partitioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd77bb2-03e0-4d71-8523-0834212a4581",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Observing Partitioned Files"
    }
   },
   "outputs": [],
   "source": [
    "# File structure\n",
    "%ls /dbfs/tmp/delta/claims_partitioned/state=CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e49260-e9d0-4851-8611-bc6a8062dc80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Inspecting First Delta Log"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the first delta log\n",
    "with open(\"/dbfs/tmp/delta/claims_partitioned/_delta_log/00000000000000000000.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        pprint(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ce90a49-8679-4a62-9bdc-cdd8b800f777",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Inspecting Partitioned Data"
    }
   },
   "outputs": [],
   "source": [
    "# File observation prior to optimizing\n",
    "spark.read.format(\"delta\").load(\n",
    "    \"dbfs:/tmp/delta/claims_partitioned/state=CA\"\n",
    ").withColumn(\"file_name\", F.element_at(F.split(F.input_file_name(), \"/\"), -1)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe21d7e-2279-4b3f-8849-a4ca8a89c2e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Optimizing Partitions"
    }
   },
   "outputs": [],
   "source": [
    "# Optimizing the parquet files\n",
    "results = spark.sql(\"OPTIMIZE delta.`/tmp/delta/claims_partitioned` ZORDER BY (claim_date)\")\n",
    "\n",
    "# Inspecting the first delta log\n",
    "with open(\"/dbfs/tmp/delta/claims_partitioned/_delta_log/00000000000000000001.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        pprint(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c6d52f3-72f3-4f14-8363-a978b861dc2f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Observing New Partitioned Files"
    }
   },
   "outputs": [],
   "source": [
    "# File structure\n",
    "%ls /dbfs/tmp/delta/claims_partitioned/state=CA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ed16cd-ac8e-4891-840e-8eb27432a9b8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Inspecting New Partitioned Data"
    }
   },
   "outputs": [],
   "source": [
    "# File inspection after to optimizing\n",
    "spark.read.format(\"delta\").load(\n",
    "    \"dbfs:/tmp/delta/claims_partitioned/state=CA\"\n",
    ").withColumn(\"file_name\", F.element_at(F.split(F.input_file_name(), \"/\"), -1)).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cc11efe-cfaa-41de-b941-bcd8b6bd92fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Why Z-Ordering ordered files by `claim_date`**\n",
    "- Z-Ordering sorts the data within each partition by the specified columns—in this case, `claim_date`.\n",
    "- This ensures that data with similar `claim_date` values ends up together in fewer files.\n",
    "\n",
    "**Benefit of Ordering**\n",
    "- When a query filters on `claim_date`, Spark reads only the small subset of files that contain the relevant dates.\n",
    "- Spark uses **file-level statistics (min/max values)** for `claim_date` to skip files that are outside the filter range. You can see these statistics in the `*.json` files of the `_delta_log`.\n",
    "- Example:\n",
    "    - id: 0 → `claim_date`: 2020-01-01 → file: `part-00000`\n",
    "    - id: 5 → `claim_date`: 2020-01-06 → same file: `part-00000`\n",
    "- Thus, file `part-00000` holds a range of sorted dates, enabling **data skipping** and faster reads.\n",
    "\n",
    "The `OPTIMIZE` command with Z-Ordering coalesces small files (e.g., many part files) into larger, fewer files and reorders the data by the Z-Order columns.\n",
    "\n",
    "**So why files are combined?**\n",
    "- Spark and Delta Lake aim to reduce the small file problem, which can hurt performance due to high metadata and shuffle overhead.\n",
    "- When `OPTIMIZE` runs, it rewrites many small files into fewer, larger files (typically 1 GB) and physically sorts the rows within those files using a space-filling curve (like Z-order).\n",
    "- If two small files both contain rows for similar `claim_date` values, they are merged into one and sorted.\n",
    "\n",
    "**Benefits of combining**\n",
    "- Improves query performance through better data skipping.\n",
    "- Reduces file system overhead and metadata load.\n",
    "- Enhances parallel read performance by creating more evenly sized files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c785b3fc-503a-494a-a8a0-6c3d99646ff6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Liquid Clustering\n",
    "---\n",
    "Next, we load the same dataset into a non-partitioned Delta table and enable Liquid Clustering with clustering on `state` and `claim_date`.\n",
    "\n",
    "**Explanation**\n",
    "- Liquid clustering avoids rigid directories by clustering within files.\n",
    "- More flexible for high-cardinality or skewed columns.\n",
    "\n",
    "**File Structure Example**:\n",
    "```\n",
    "/tmp/delta/claims_liquid/\n",
    "├── part-0000.snappy.parquet\n",
    "├── part-0001.snappy.parquet\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "796cd11a-1d13-444a-9ee3-fcdb809dab98",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Removing old data"
    }
   },
   "outputs": [],
   "source": [
    "# Removing the old data for new tutorial runs\n",
    "%rm -rf /dbfs/tmp/delta/claims_liquid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03d9851-26ca-4de6-98bb-7d75ebed9cb7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Creating Liquid Clustering"
    }
   },
   "outputs": [],
   "source": [
    "claims_df.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/tmp/delta/claims_liquid\")\n",
    "\n",
    "spark.sql(\"ALTER TABLE delta.`/tmp/delta/claims_liquid` CLUSTER BY (state, claim_date)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c570f344-7f92-4383-bab3-51a15e5f9a07",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Observing  Parquet Files"
    }
   },
   "outputs": [],
   "source": [
    "# No partitioned structure\n",
    "%ls /dbfs/tmp/delta/claims_liquid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b27f002-0435-4afc-bc8a-50103ae5c4f5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Inspecting  Data"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"dbfs:/tmp/delta/claims_liquid/\").withColumn(\"file_name\", F.element_at(F.split(F.input_file_name(), \"/\"), -1)).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b0a2a9e-e992-4744-82c6-8c139eab73ec",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Observing Delta Log Files"
    }
   },
   "outputs": [],
   "source": [
    "%ls /dbfs/tmp/delta/claims_liquid/_delta_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8366047d-fd43-42d9-aed1-642b8a07ee9d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Inspecting Latest Delta Log File"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting the last delta log\n",
    "with open(\"/dbfs/tmp/delta/claims_partitioned/_delta_log/00000000000000000003.json\", \"r\") as f:\n",
    "    for line in f:\n",
    "        pprint(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd618cf2-2691-46f2-a50b-93a2bffcccab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Performance Comparison\n",
    "---\n",
    "Measure performance using Spark queries with `%timeit` for skipping benefits.\n",
    "\n",
    "**Explanation**\n",
    "- Try filtering by `state='CA' AND claim_date='2020-06-01'`\n",
    "- Compare scan statistics from both tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b1d970e-34ea-4339-bb7f-95b66e1a4f99",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Narrow dependecy transformation for partition"
    }
   },
   "outputs": [],
   "source": [
    "%timeit spark.read.format(\"delta\").load(\"/tmp/delta/claims_partitioned\").filter(\"state = 'CA' AND claim_date = '2020-01-06'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c333aa72-a1f6-490c-87f2-e8b7c8e9a4f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Narrow dependency transformation for liquid"
    }
   },
   "outputs": [],
   "source": [
    "%timeit spark.read.format(\"delta\").load(\"/tmp/delta/claims_liquid\").filter(\"state = 'CA' AND claim_date = '2020-01-06'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a890ce-d3fa-42b2-bc11-a83a08ea6ee6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Wide dependency transformation for partition"
    }
   },
   "outputs": [],
   "source": [
    "%timeit spark.read.format(\"delta\").load(\"dbfs:/tmp/delta/claims_partitioned/\") \\\n",
    "    .groupBy(\"state\", \"claim_date\") \\\n",
    "    .agg(F.count(\"claim_id\").alias(\"total_claims\")) \\\n",
    "    .withColumn(\"file_name\", F.element_at(F.split(F.input_file_name(), \"/\"), -1)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00525bda-0864-4fe2-8be7-65922cc6845a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "- Wide dependency transformation for liquid"
    }
   },
   "outputs": [],
   "source": [
    "%timeit spark.read.format(\"delta\").load(\"dbfs:/tmp/delta/claims_liquid/\") \\\n",
    "    .groupBy(\"state\", \"claim_date\") \\\n",
    "    .agg(F.count(\"claim_id\").alias(\"total_claims\")) \\\n",
    "    .withColumn(\"file_name\", F.element_at(F.split(F.input_file_name(), \"/\"), -1)) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b574e01c-2fde-4717-b3c8-ea84f91e89eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Conclusion\n",
    "---\n",
    "Use Partitioning + Z-Ordering for low-cardinality, evenly distributed columns. Use Liquid Clustering for high-cardinality or skewed distributions.\n",
    "\n",
    "**Explanation**\n",
    "- Partitioning is directory-based and static.\n",
    "- Liquid Clustering is more dynamic and easier to maintain.\n",
    "- Choose based on data distribution and access patterns.\n",
    "\n",
    "**Limitations of liquid clustering**\n",
    "- Statistucs are collected for the first 32 columns in the delta table, clustering outside this limit will not work.\n",
    "- You can only cluster for up to 4 columns.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3162583433136062,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "databricks-partitioning-vs-liquid-clustering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
