{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Deploying Gemma2 9B Model with FastAPI\n",
    "\n",
    "This tutorial will guide you through deploying the Gemma2 9B model (from Hugging Face) using FastAPI. We will build a FastAPI application to serve the model, handle requests, and return predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Install Required Libraries\n",
    "We need to install the necessary libraries for our application, including `fastapi`, `uvicorn`, and `transformers`.\n",
    "\n",
    "**Explanation**\n",
    "- `fastapi`: Framework for building APIs.\n",
    "- `uvicorn`: ASGI server to run the FastAPI app.\n",
    "- `transformers`: For working with the Gemma2 9B model from Hugging Face.\n",
    "- `torch`: For PyTorch-based model inference.\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ./fastapi-env/bin/activate && pip install -q transformers torch python-dotenv 'accelerate>=0.26.0' websockets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from helper import FastAPIServer\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import websockets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Gemma2 9B Model\n",
    "We'll use the transformers library to load the Gemma2 9B model and its tokenizer.\n",
    "\n",
    "**Explanation**\n",
    "* The `AutoModelForCausalLM` class loads the pre-trained Gemma2 model.\n",
    "* The `AutoTokenizer` handles input text tokenization.\n",
    "* Use `from_pretrained()` to load the model and tokenizer from Hugging Face.\n",
    "\n",
    "For secure access to private or restricted Hugging Face models, we will store the authentication token in a .env file. Python's os and dotenv libraries will retrieve the token at runtime.\n",
    "\n",
    "**Steps to Securely Store the Token**\n",
    "\n",
    "> You must have a Hugging Face account and a token to access private models. Since user interfaces changes frequently, please refer to the Hugging Face documentation for the most up-to-date instructions on how to obtain a token. You wil also need to aggregate the model's name and the organization name to access the model.\n",
    "\n",
    "1. Create a .env file in your project directory.\n",
    "2. Add the Hugging Face token to the .env file.\n",
    "3. Use the dotenv library to load the token into your application.\n",
    "\n",
    "**Explanation**\n",
    "- The `.env` file keeps sensitive information like tokens separate from your codebase.\n",
    "- `os.getenv()` retrieves the token without hardcoding it in your script.\n",
    "- `use_auth_token` ensures the Hugging Face API uses the provided token for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA, MPS, and fallback to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hugging Face token and save directory\n",
    "GEMA_TOKEN = os.getenv(\"GEMMA_TOKEN\")\n",
    "save_directory = \"saved_model\"\n",
    "\n",
    "# Temporary code to prevent issues with forked processes\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Try loading the model and tokenizer\n",
    "try:\n",
    "    print(\"Attempting to load the model from the saved directory...\")\n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(save_directory).to(device)\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    print(\"Model and tokenizer loaded successfully from the saved directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from saved directory: {e}\")\n",
    "    print(\"Downloading model and tokenizer from Hugging Face...\")\n",
    "    \n",
    "    # Download and save the model and tokenizer\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", use_auth_token=GEMA_TOKEN)\n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-9b-it\",\n",
    "        torch_dtype=torch.float32,\n",
    "        use_auth_token=GEMA_TOKEN\n",
    "    ).to(device)\n",
    "\n",
    "    # Save to the directory\n",
    "    loaded_model.save_pretrained(save_directory)\n",
    "    loaded_tokenizer.save_pretrained(save_directory)\n",
    "    print(\"Model and tokenizer downloaded and saved successfully.\")\n",
    "\n",
    "# Test functionality\n",
    "sample_text = \"Once upon a time\"\n",
    "inputs = loaded_tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "outputs = loaded_model.generate(inputs[\"input_ids\"])\n",
    "print(loaded_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Gemma2 API: Text Generation with FastAPI & Transformers**  \n",
    "\n",
    "This script sets up an AI-powered text generation API using FastAPI and a pre-trained transformer model. It provides a simple interface for generating text based on user input. This script will show you how to make a simple API that can be used with curl, Postman, or any other HTTP client. Later, we will focus on deploying a more intuitive and user-friendly interface within the jupyter notebook.\n",
    "\n",
    "#### **1Ô∏è‚É£ Model Initialization**  \n",
    "- Loads a pre-trained language model (`AutoModelForCausalLM`) and its tokenizer from a saved directory (`saved_model`).  \n",
    "- Ensures the model is ready for inference as soon as the API starts.  \n",
    "\n",
    "#### **2Ô∏è‚É£ FastAPI Setup**  \n",
    "- Initializes FastAPI (`app = FastAPI()`) to provide an interface for text generation.  \n",
    "- Defines structured request and response models using Pydantic as an example of its usage (another tutorial will cover Pydantic in more detail):  \n",
    "  - `RequestModel`: Accepts a prompt and an optional `max_length` for response generation.  \n",
    "  - `ResponseModel`: Returns generated text in a structured format.  \n",
    "\n",
    "#### **3Ô∏è‚É£ API Endpoints**  \n",
    "- **`/` (GET)** - Returns a simple status message confirming the API is running.  \n",
    "- **`/predict` (POST)** - Generates text based on the user's prompt using the AI model.  \n",
    "\n",
    "#### **4Ô∏è‚É£ Text Generation Process**  \n",
    "1. Receives a JSON request containing a user prompt and optional `max_length`.  \n",
    "2. Encodes the input using the tokenizer.  \n",
    "3. Passes it to the AI model, which generates a continuation.  \n",
    "4. Decodes and returns the output in JSON format.  \n",
    "\n",
    "#### **Key Benefits**  \n",
    "- Fast and lightweight API using FastAPI for quick response times.  \n",
    "- Easy integration with any frontend or automation system.  \n",
    "- Customizable response length for better control over AI output.  \n",
    "- Efficient model inference running asynchronously.  \n",
    "\n",
    "#### **Next Steps**  \n",
    "1. Run the API server:  \n",
    "   ```sh\n",
    "   uvicorn main:app --host 127.0.0.1 --port 8080\n",
    "   ```  \n",
    "   In this case this is wrapped inside the following line of code:  \n",
    "   ```python\n",
    "    port = 8080\n",
    "    fastapi = FastAPIServer(port)\n",
    "    fastapi.run()\n",
    "   ```\n",
    "2. Test in a browser by visiting `http://127.0.0.1:8080/` to check if the API is running.  \n",
    "3. Send a prediction request using `cURL` or Postman:  \n",
    "   ```sh\n",
    "   curl -X 'POST' 'http://127.0.0.1:8080/predict' \\\n",
    "        -H 'Content-Type: application/json' \\\n",
    "        -d '{\"prompt\": \"Hello, AI!\", \"max_length\": 50}'\n",
    "   ```  \n",
    "4. Integrate the API into chatbots, content generation tools, or automation workflows.  \n",
    "\n",
    "This API provides a simple yet powerful way to interact with AI-generated text, making it ideal for NLP applications.\n",
    "\n",
    "Tip `!kill -9 $(lsof -t -i :8080)` to kill the server if it remains open and you are having conflicts with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "save_directory = \"saved_model\"\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define request and response models\n",
    "class RequestModel(BaseModel):\n",
    "    prompt: str\n",
    "    max_length: int = 50  # Optional: max tokens in the response\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    generated_text: str\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Gemma2 API is running\"}\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict\", response_model=ResponseModel)\n",
    "async def predict(request: RequestModel):\n",
    "    inputs = loaded_tokenizer(request.prompt, return_tensors=\"pt\")\n",
    "    outputs = loaded_model.generate(inputs[\"input_ids\"], max_length=request.max_length)\n",
    "    generated_text = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"generated_text\": generated_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 8080\n",
    "fastapi = FastAPIServer(port)\n",
    "fastapi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://127.0.0.1:8080/predict\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\"prompt\": \"Hello, AI\", \"max_length\": 50}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. WebSocket-Based AI Chat Server with FastAPI\n",
    "\n",
    "In this section we will create a wbsocket/fastapi server to serve the model in real-time in the local machine. You should have a basic understanding of FastAPI and Websockets to follow along and a machone with a GPU to run the model.\n",
    "\n",
    "### Server\n",
    "\n",
    "This script sets up a **WebSocket server using FastAPI** to handle real-time communication with an AI-powered assistant. Below is a structured breakdown of its functionality:\n",
    "\n",
    "#### **1Ô∏è‚É£ Logger Configuration**\n",
    "- Configures **critical-level logging** to reduce unnecessary log output.\n",
    "\n",
    "#### **2Ô∏è‚É£ AI Model Loading**\n",
    "- Loads a pre-trained **transformer model** (`AutoModelForCausalLM`) and tokenizer from a saved directory.\n",
    "- Defines a **system prompt** to guide AI responses.\n",
    "\n",
    "#### **3Ô∏è‚É£ WebSocket Handling (`/chat` endpoint)**\n",
    "- Accepts **WebSocket connections** and maintains active connections in a list.\n",
    "- Listens for incoming messages:\n",
    "  - **Ignores \"ping\" messages** (heartbeat signals from clients). These pings help keep the connection alive.\n",
    "  - **Processes user input** using the AI model and system prompt.\n",
    "  - **Runs model inference asynchronously** with `asyncio.to_thread()` to prevent blocking.\n",
    "  - **Sends AI responses** back to all connected clients.\n",
    "\n",
    "#### **4Ô∏è‚É£ Heartbeat Mechanism**\n",
    "- A separate `model_heartbeats()` task **sends periodic \"processing...\" messages** every 5 seconds.\n",
    "- Ensures WebSocket remains open while the AI model generates a response.\n",
    "- The heartbeat task is **canceled once the response is ready** to avoid overprocessing.\n",
    "\n",
    "#### **5Ô∏è‚É£ Graceful Disconnection Handling**\n",
    "- **Removes clients from the active connections list** when they disconnect.\n",
    "- Catches **WebSocket errors** to prevent crashes.\n",
    "\n",
    "This example demonstrates how to **integrate AI models with WebSocket servers** for interactive applications, chatbots, and more. Feel free to customize the AI model, system prompts, and WebSocket handling to suit your use case! ü§ñüåê"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "import asyncio\n",
    "import logging\n",
    "from fastapi import FastAPI, WebSocket\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import List\n",
    "\n",
    "# LOGGER CONFIGURATION\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    level=logging.CRITICAL,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# MODEL CONFIGURATION\n",
    "# Load the saved model and tokenizer\n",
    "save_directory = \"saved_model\"\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Define system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful AI assistant. Answer concisely and clearly. Your name is not Gemma. It is SilverAIWolf.\n",
    "\"\"\"\n",
    "\n",
    "app = FastAPI()\n",
    "connections: List[WebSocket] = []\n",
    "\n",
    "@app.websocket(\"/chat\")\n",
    "async def websocket_endpoint(websocket: WebSocket):\n",
    "    \"\"\"Handles WebSocket connections.\"\"\"\n",
    "    await websocket.accept()\n",
    "    connections.append(websocket)\n",
    "    client_ip = websocket.client.host\n",
    "    logger.info(f\"Client {client_ip} connected. Total connections: {len(connections)}\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            data = await websocket.receive_text()\n",
    "            logger.debug(data)\n",
    "            if data == \"ping\":  # Ignore heartbeat messages\n",
    "                logger.debug(f\"Received heartbeat from {client_ip}\")\n",
    "                continue\n",
    "\n",
    "            # Start heartbeat task\n",
    "            model_heartbeat_task = asyncio.create_task(model_heartbeats(websocket))\n",
    "\n",
    "            # Run the model asynchronously using `asyncio.to_thread`\n",
    "            full_prompt = f\"{SYSTEM_PROMPT}\\nUser: {data}\\nAI:\"\n",
    "            inputs = loaded_tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Process the model response in a separate thread to avoid blocking the model heartbeats\n",
    "            response = await asyncio.to_thread(\n",
    "                lambda: generate_response(loaded_model, loaded_tokenizer, inputs)\n",
    "            )\n",
    "\n",
    "            # Stop heartbeats and send the final response\n",
    "            model_heartbeat_task.cancel()\n",
    "\n",
    "            # Send the response to all connected clients\n",
    "            for conn in connections:\n",
    "                await conn.send_text(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"WebSocket error with {client_ip}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        connections.remove(websocket)\n",
    "        logger.info(f\"Client {client_ip} disconnected. Total connections: {len(connections)}\")\n",
    "\n",
    "\n",
    "async def model_heartbeats(websocket):\n",
    "    logger.debug(\"Model heartbeat started\")\n",
    "    counter = 0\n",
    "    while True:\n",
    "        await asyncio.sleep(5)\n",
    "        try:\n",
    "            await websocket.send_text(\"processing\" + \".\" * ((counter % 30) + 1))\n",
    "            counter += 1\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Heartbeat error: {e}\")\n",
    "            break\n",
    "\n",
    "\n",
    "def generate_response(model, tokenizer, inputs):\n",
    "    \"\"\"Runs the model processing in a separate thread to avoid blocking.\"\"\"\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=500)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text.split(\"AI:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client\n",
    "\n",
    "### **Interactive WebSocket Chat Client for SilverAIWolf**\n",
    "\n",
    "This script sets up an **interactive chat client** that connects to the **SilverAIWolf WebSocket server**. It enables real-time communication with the AI assistant while ensuring smooth user experience.\n",
    "\n",
    "\n",
    "### **1Ô∏è‚É£ WebSocket Connection**\n",
    "- Establishes a **WebSocket connection** with the server at `ws://127.0.0.1:8080/chat` using the `websockets` library.\n",
    "- The connection remains **open for continuous interaction** with the AI assistant.\n",
    "\n",
    "\n",
    "### **2Ô∏è‚É£ Unique Welcome Screen**\n",
    "- Displays an **ASCII logo** and an **engaging introduction**.\n",
    "- Highlights key features:\n",
    "  - **Instant AI-powered responses**.\n",
    "  - **Simple commands** (`exit` to close the chat).\n",
    "  - **Immersive chat experience**.\n",
    "\n",
    "\n",
    "### **3Ô∏è‚É£ User Input Handling**\n",
    "- Uses `asyncio.to_thread(input, \"You: \")` to **avoid blocking the event loop**, allowing WebSocket communication to run efficiently.\n",
    "- Ensures **graceful exit** when the user types `\"exit\"` or `\"bye\"`.\n",
    "\n",
    "\n",
    "### **4Ô∏è‚É£ Real-Time Message Processing**\n",
    "- Sends the **user's message** to the WebSocket server.\n",
    "- **Waits for responses** while handling:\n",
    "  - **\"Processing...\" messages**: Keeps the chat interactive during AI processing.\n",
    "  - **Final AI responses**: Displays neatly formatted output.\n",
    "\n",
    "\n",
    "### **5Ô∏è‚É£ User-Friendly Output**\n",
    "- **\"Processing...\" updates** overwrite the same console line instead of spamming multiple lines.\n",
    "- **AI responses** are displayed in a structured format:\n",
    "  ```\n",
    "  SilverAIWolf: <AI Response>\n",
    "  ------------------------------------------------------------\n",
    "  ```\n",
    "- **Farewell Message**: Displays `\"Goodbye, wanderer! Until next time. üê∫‚ú®\"` when exiting.\n",
    "\n",
    "\n",
    "### **‚úÖ Key Benefits**\n",
    "| **Feature** | **Description** |\n",
    "|------------|----------------|\n",
    "| ‚úÖ **Seamless WebSocket Communication** | Ensures smooth interaction with the AI assistant. |\n",
    "| ‚úÖ **Interactive UI** | Custom ASCII branding and clear instructions. |\n",
    "| ‚úÖ **Efficient Message Handling** | Prevents UI lag using `asyncio.to_thread()`. |\n",
    "| ‚úÖ **Real-Time Processing Updates** | Shows `\"processing...\"` instead of making the user wait silently. |\n",
    "| ‚úÖ **Graceful Disconnection** | Properly exits and displays a friendly goodbye message. |\n",
    "\n",
    "\n",
    "### **üöÄ Next Steps**\n",
    "1. **Run the script** and test chat interactions.\n",
    "2. **Ensure the WebSocket server is running (`main.py`)**.\n",
    "3. **Try different inputs** to see AI responses in action.\n",
    "4. **Enjoy chatting with SilverAIWolf!** üê∫‚ú®\n",
    "\n",
    "---\n",
    "\n",
    "This tutorial ensures **a smooth, engaging, and efficient** WebSocket chat experience with **SilverAIWolf**! üöÄüî•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import websockets\n",
    "\n",
    "async def chat():\n",
    "    uri = \"ws://127.0.0.1:8080/chat\"\n",
    "    \n",
    "    async with websockets.connect(uri) as websocket:\n",
    "        print(\"\"\"\n",
    "‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ïó‚ñë‚ñë‚ñë‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñà‚ñà‚ïó‚ñë‚ñà‚ñà‚ïó‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ïó‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñà‚ñà‚ïó‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó\n",
    "‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñà‚ñà‚ïó‚ñë‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë‚ïö‚ñà‚ñà‚ïó‚ñë‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñë‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñë\n",
    "‚ñë‚ïö‚ïê‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñë‚ïö‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñë‚ñë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ñà‚ñà‚ïë‚ñë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïù‚ñë‚ñë\n",
    "‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñë‚ñë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñë‚ñë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñë‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñë‚ñë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñë‚ïö‚ñà‚ñà‚ïî‚ïù‚ñë‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ñë‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ñë‚ñë‚ñë‚ïö‚ïê‚ïù‚ñë‚ñë‚ñë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù‚ñë‚ñë‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ñë‚ñë‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù‚ñë‚ñë‚ñë‚ïö‚ïê‚ïù‚ñë‚ñë‚ñë‚ïö‚ïê‚ïù‚ñë‚ñë‚ñë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïù‚ñë‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù‚ñë‚ñë‚ñë‚ñë‚ñë\n",
    "\n",
    "Welcome to SilverAIWolf Chat! üê∫‚ö°\n",
    "Your intelligent AI assistant, always ready to help!\n",
    "\n",
    "üîπ Type your messages below and get instant AI-powered responses.\n",
    "üîπ Type `bye` or `exit` to close the chat.\n",
    "üîπ AI is listening... Let's chat! üöÄ\n",
    "        \"\"\")\n",
    "\n",
    "        while True:\n",
    "            msg = await asyncio.to_thread(input, \"\\nYou: \")  # Blocks input until response is received\n",
    "            if msg.lower() in [\"bye\", \"exit\"]:\n",
    "                print(\"\\nSilverAIWolf: Goodbye, wanderer! Until next time. üê∫‚ú®\\n\")\n",
    "                break\n",
    "            \n",
    "            await websocket.send(msg)  # Send user input\n",
    "            \n",
    "            while True:\n",
    "                response = await websocket.recv()\n",
    "                if \"processing\" in response:\n",
    "                    print(response, end=\"\\r\", flush=True)  # Overwrites console with \"processing...\"\n",
    "                else:\n",
    "                    print(f\"\\nSilverAIWolf: {response}\\n\", end=\"-\"*60)  # Prints final response\n",
    "                    break  # Exit processing loop before taking new input\n",
    "\n",
    "# Run the chat function asynchronously\n",
    "await chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SilverAIWolf (FastAPI)",
   "language": "python",
   "name": "fastapi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
