{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Deploying Gemma2 9B Model with FastAPI\n",
    "\n",
    "This tutorial will guide you through deploying the Gemma2 9B model (from Hugging Face) using FastAPI. We will build a FastAPI application to serve the model, handle requests, and return predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Install Required Libraries\n",
    "We need to install the necessary libraries for our application, including `fastapi`, `uvicorn`, and `transformers`.\n",
    "\n",
    "**Explanation**\n",
    "- `fastapi`: Framework for building APIs.\n",
    "- `uvicorn`: ASGI server to run the FastAPI app.\n",
    "- `transformers`: For working with the Gemma2 9B model from Hugging Face.\n",
    "- `torch`: For PyTorch-based model inference.\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ./fastapi-env/bin/activate && pip install -q transformers torch python-dotenv 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import torch\n",
    "from helper import FastAPIServer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load the Gemma2 9B Model\n",
    "We'll use the transformers library to load the Gemma2 9B model and its tokenizer.\n",
    "\n",
    "**Explanation**\n",
    "* The `AutoModelForCausalLM` class loads the pre-trained Gemma2 model.\n",
    "* The `AutoTokenizer` handles input text tokenization.\n",
    "* Use `from_pretrained()` to load the model and tokenizer from Hugging Face.\n",
    "\n",
    "For secure access to private or restricted Hugging Face models, we will store the authentication token in a .env file. Python's os and dotenv libraries will retrieve the token at runtime.\n",
    "\n",
    "**Steps to Securely Store the Token**\n",
    "\n",
    "> You must have a Hugging Face account and a token to access private models. Since user interfaces changes frequently, please refer to the Hugging Face documentation for the most up-to-date instructions on how to obtain a token. You wil also need to aggregate the model's name and the organization name to access the model.\n",
    "\n",
    "1. Create a .env file in your project directory.\n",
    "2. Add the Hugging Face token to the .env file.\n",
    "3. Use the dotenv library to load the token into your application.\n",
    "\n",
    "**Explanation**\n",
    "- The `.env` file keeps sensitive information like tokens separate from your codebase.\n",
    "- `os.getenv()` retrieves the token without hardcoding it in your script.\n",
    "- `use_auth_token` ensures the Hugging Face API uses the provided token for authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA, MPS, and fallback to CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Hugging Face token and save directory\n",
    "GEMA_TOKEN = os.getenv(\"GEMMA_TOKEN\")\n",
    "save_directory = \"saved_model\"\n",
    "\n",
    "# Temporary code to prevent issues with forked processes\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Try loading the model and tokenizer\n",
    "try:\n",
    "    print(\"Attempting to load the model from the saved directory...\")\n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(save_directory).to(device)\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "    print(\"Model and tokenizer loaded successfully from the saved directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to load model from saved directory: {e}\")\n",
    "    print(\"Downloading model and tokenizer from Hugging Face...\")\n",
    "    \n",
    "    # Download and save the model and tokenizer\n",
    "    loaded_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b-it\", use_auth_token=GEMA_TOKEN)\n",
    "    loaded_model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"google/gemma-2-9b-it\",\n",
    "        torch_dtype=torch.float32,\n",
    "        use_auth_token=GEMA_TOKEN\n",
    "    ).to(device)\n",
    "\n",
    "    # Save to the directory\n",
    "    loaded_model.save_pretrained(save_directory)\n",
    "    loaded_tokenizer.save_pretrained(save_directory)\n",
    "    print(\"Model and tokenizer downloaded and saved successfully.\")\n",
    "\n",
    "# Test functionality\n",
    "sample_text = \"Once upon a time\"\n",
    "inputs = loaded_tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "outputs = loaded_model.generate(inputs[\"input_ids\"])\n",
    "print(loaded_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a FastAPI Application\n",
    "Set up a basic FastAPI application to define endpoints for model inference.\n",
    "\n",
    "**Explanation**\n",
    "- Define a root endpoint to verify the API is running.\n",
    "- Create a `POST` endpoint /predict for model inference.\n",
    "- Use `pydantic` for request/response models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "save_directory = \"saved_model\"\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Initialize FastAPI app\n",
    "app = FastAPI()\n",
    "\n",
    "# Define request and response models\n",
    "class RequestModel(BaseModel):\n",
    "    prompt: str\n",
    "    max_length: int = 50  # Optional: max tokens in the response\n",
    "\n",
    "class ResponseModel(BaseModel):\n",
    "    generated_text: str\n",
    "\n",
    "# Root endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Gemma2 API is running\"}\n",
    "\n",
    "# Prediction endpoint\n",
    "@app.post(\"/predict\", response_model=ResponseModel)\n",
    "async def predict(request: RequestModel):\n",
    "    inputs = loaded_tokenizer(request.prompt, return_tensors=\"pt\")\n",
    "    outputs = loaded_model.generate(inputs[\"input_ids\"], max_length=request.max_length)\n",
    "    generated_text = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return {\"generated_text\": generated_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 $(lsof -t -i :8081)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port = 8081\n",
    "fastapi = FastAPIServer(port)\n",
    "fastapi.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \"http://127.0.0.1:8081/predict\" \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\"prompt\": \"Hello\", \"max_length\": 100}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Real-time Model Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "import asyncio\n",
    "from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Depends\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"websocket\")\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "save_directory = \"saved_model\"\n",
    "loaded_model = AutoModelForCausalLM.from_pretrained(save_directory)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
    "\n",
    "# Define system prompt\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a helpful AI assistant. Answer concisely and clearly. Your name is not Gemma. It is SilverAIWolf.\n",
    "\"\"\"\n",
    "\n",
    "# Create an instance of FastAPI\n",
    "app = FastAPI()\n",
    "\n",
    "# Add CORS middleware for frontend-backend communication\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],  # Change to specific domains in production\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class ConnectionManager:\n",
    "    def __init__(self):\n",
    "        self.active_connections: list[WebSocket] = []\n",
    "\n",
    "    async def connect(self, websocket: WebSocket):\n",
    "        await websocket.accept()\n",
    "        self.active_connections.append(websocket)\n",
    "        logger.info(f\"New connection established. Active connections: {len(self.active_connections)}\")\n",
    "\n",
    "    def disconnect(self, websocket: WebSocket):\n",
    "        if websocket in self.active_connections:\n",
    "            self.active_connections.remove(websocket)\n",
    "            logger.info(f\"Connection removed. Active connections: {len(self.active_connections)}\")\n",
    "\n",
    "    async def broadcast(self, message: str):\n",
    "        # logger.info(f\"Broadcasting message: {message}\")\n",
    "        for connection in self.active_connections:\n",
    "            try:\n",
    "                await connection.send_text(message)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error broadcasting to connection: {e}\")\n",
    "                self.disconnect(connection)\n",
    "\n",
    "# Create an instance of the ConnectionManager\n",
    "manager = ConnectionManager()\n",
    "\n",
    "@app.websocket(\"/ws/chat\")\n",
    "async def websocket_chat_endpoint(websocket: WebSocket):\n",
    "    await manager.connect(websocket)\n",
    "    try:\n",
    "        while True:  \n",
    "            try:\n",
    "                data = await asyncio.wait_for(websocket.receive_text(), timeout=60)  \n",
    "                if data == 'ping':\n",
    "                    logger.info(\"Server received the ping\")\n",
    "                    \n",
    "                    # Respond to ping requests to keep the connection alive\n",
    "                    await websocket.send_text('pong')\n",
    "                    continue\n",
    "                    \n",
    "            except asyncio.TimeoutError:\n",
    "                logger.info(\"Connection timeout.\")\n",
    "                break\n",
    "\n",
    "            full_prompt = f\"{SYSTEM_PROMPT}\\nUser: {data}\\nAI:\"\n",
    "            inputs = loaded_tokenizer(full_prompt, return_tensors=\"pt\")\n",
    "\n",
    "            # Respond with the entire reply\n",
    "            outputs = loaded_model.generate(inputs[\"input_ids\"], max_length=500)\n",
    "            generated_text = loaded_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response = response = generated_text.split(\"AI:\")[-1].strip()\n",
    "            await manager.broadcast(response)\n",
    "                \n",
    "    except WebSocketDisconnect:\n",
    "        manager.disconnect(websocket)\n",
    "        await manager.broadcast(\"A user has left the chat.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in WebSocket handler: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import websockets\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "PING_INTERVAL = 10  # Send a ping every 30 seconds\n",
    "\n",
    "async def send_ping(websocket):\n",
    "    \"\"\"Periodically send a ping message to keep the connection alive.\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            await asyncio.sleep(PING_INTERVAL)\n",
    "            print(\"[Client] Sending ping...\")\n",
    "            await websocket.send(\"ping\")\n",
    "            print(\"ping sent!\")\n",
    "        except (websockets.exceptions.ConnectionClosed, asyncio.CancelledError):\n",
    "            print(\"[Client] Stopping ping task: Connection closed.\")\n",
    "            break\n",
    "\n",
    "async def chat_client(port):\n",
    "    \"\"\" WebSocket chat client with ping handling and reconnection \"\"\"\n",
    "    uri = f\"ws://localhost:{port}/ws/chat\"\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            async with websockets.connect(uri) as websocket:\n",
    "                print(\"Connected to the chat server. Type 'exit' to quit.\")\n",
    "                \n",
    "                # Start sending pings in the background\n",
    "                asyncio.create_task(send_ping(websocket))\n",
    "\n",
    "                while True:\n",
    "                    message = input(\"You: \")\n",
    "                    \n",
    "                    if message.lower() == \"exit\":\n",
    "                        print(\"Exiting chat...\")\n",
    "                        return\n",
    "                    \n",
    "                    await websocket.send(message)\n",
    "                    response = await websocket.recv()\n",
    "\n",
    "                    # Check if the response is a pong\n",
    "                    if response == \"pong\":\n",
    "                        print(\"[Client] Received pong from server (connection is alive)\")\n",
    "                        continue\n",
    "\n",
    "                    print(f\"SilverAIWolf: {response}\")\n",
    "\n",
    "        except (websockets.exceptions.ConnectionClosed, ConnectionRefusedError) as e:\n",
    "            print(\"[Client] Connection lost. Reconnecting in 5 seconds...\")\n",
    "            await asyncio.sleep(5)  # Wait before retrying\n",
    "\n",
    "# Run the chat client\n",
    "asyncio.get_event_loop().run_until_complete(chat_client(port))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
