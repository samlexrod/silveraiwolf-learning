{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59788364-4f6d-488a-8329-fa20e9d87d0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks GitIngest and Chunking ETL\n",
    "\n",
    "## Get ready to put your Data Engineering Hat! üß¢\n",
    "\n",
    "This tutorial walks through setting up GitIngest in Databricks, extracting raw data from a repository, processing it into structured chunks, and finally loading it into llm.rag.silveraiwolf_learning for LLM-based RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "```\n",
    "PACKAGE VERSIONS:\n",
    "- langchain==0.3.14   \n",
    "- re==2.2.1\n",
    "- pyspark==3.5.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4cdad653-0169-4ee8-8bb5-7ec4bded5c60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "import re\n",
    "from functools import reduce\n",
    "from pyspark.sql.types import StringType, StructField, StructType, ArrayType, IntegerType\n",
    "from pyspark.sql.functions import udf, col, array_size, posexplode, concat, lit\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3345c41a-9d47-4249-98e4-86006fd33ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"\"\"\n",
    "PACKAGE VERSIONS:\n",
    "- langchain=={langchain.__version__}   \n",
    "- re=={re.__version__}\n",
    "- pyspark=={pyspark.__version__}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5acd4add-2e9b-4e94-8273-358a09f6e6bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 1. Creating the LLM Catalog and Database for RAG\n",
    "---\n",
    "Before implementing **Retrieval-Augmented Generation (RAG)** with Large Language Models (LLMs), we need to establish a **catalog and database** in Databricks. These organizational structures allow us to efficiently store, manage, and retrieve knowledge sources that enhance LLM responses.\n",
    "\n",
    "In **Databricks**, a **catalog** and a **database** play a crucial role in structuring and governing **retrievable data** for LLM workflows. Here's how:\n",
    "\n",
    "### 1. **Catalog (High-Level Data Governance)**\n",
    "   - A **catalog** serves as the **top-level container** for organizing data assets in Databricks.\n",
    "   - It provides a **centralized metadata store** that ensures **access control, security, and compliance** for RAG-based knowledge retrieval.\n",
    "   - In **Unity Catalog**, it facilitates structured **data lineage tracking**, allowing LLMs to retrieve and reference the most **accurate, versioned, and curated data**.\n",
    "   - Think of a catalog as a **collection of databases** containing the knowledge sources for RAG.\n",
    "\n",
    "   **Example:**\n",
    "   ```sql\n",
    "   CREATE CATALOG llm_catalog;\n",
    "   ```\n",
    "\n",
    "### 2. **Database (Schema for Knowledge Management)**\n",
    "  - A **database (schema)** inside a catalog organizes **tables, vector embeddings, and indexed documents** used in RAG.\n",
    "  - It helps manage **structured (tabular), semi-structured (JSON), and unstructured (text, embeddings)** data for efficient retrieval.\n",
    "  - By structuring LLM knowledge bases into a database schema, we ensure **efficient lookups, version control, and traceability** of referenced knowledge.\n",
    "\n",
    "  **Examples:**\n",
    "  \n",
    "  - Unified approach\n",
    "    ```sql\n",
    "    CREATE DATABASE llm.rag;\n",
    "    ```\n",
    "  - Segmented approach\n",
    "    ```sql\n",
    "    CREATE DATABASE llm.knowledge;\n",
    "    ```\n",
    "\n",
    "#### 2.1 Using a Single Knowledge Base Table (Unified Approach)\n",
    "\n",
    "‚úÖ Best for: Generalized knowledge retrieval, simple governance, and ease of management.\n",
    "Implementation:\n",
    "- Store all knowledge sources (structured, semi-structured, and unstructured) in one table.\n",
    "- Use columns to differentiate document types, sources, or categories.\n",
    "- Implement vector embeddings within the same table for efficient similarity search.\n",
    "\n",
    "Example Table Schema: (`llm.rag.knowledge_base`)\n",
    "| chunk_id(BIGINT) | document_text(STRING)  | source_type(STRING)     | metadata (JSON)         | vector_embedding (ARRAY<Float>) |\n",
    "|----|--------------------------------|-----------------|-------------------------|-------------|\n",
    "| 1  | \"Databricks simplifies AI...\"  | Documentation   | {\"category\": \"AI\"}      | [0.12, 0.34, ...]               |\n",
    "| 2  | \"Stock market trends in 2024...\" | Finance Reports | {\"category\": \"Finance\"} | [0.87, 0.45, ...]               |\n",
    "| 3  | \"Python's async features...\"   | Blog Article    | {\"category\": \"Tech\"}    | [0.65, 0.23, ...]               |\n",
    "\n",
    "Pros of a Single Table:\n",
    "-   ‚úî Simple query structure ‚Äì Easier to maintain and retrieve data.\n",
    "-   ‚úî Centralized management ‚Äì All knowledge exists in one place, reducing redundancy.\n",
    "-   ‚úî Efficient vector search ‚Äì Unified embeddings allow for cross-domain retrieval.\n",
    "\n",
    "Cons of a Single Table:\n",
    "-   ‚ùå Potential performance bottlenecks ‚Äì Large datasets may slow down retrieval.\n",
    "-   ‚ùå Complex access control ‚Äì If different teams need different permissions, fine-grained control is harder.\n",
    "-   ‚ùå Schema complexity ‚Äì Handling different data types and sources in a unified structure requires robust schema design.\n",
    "\n",
    "#### 2.2 Using Multiple Knowledge Base Tables (Segmented Approach)\n",
    "\n",
    "‚úÖ Best for: Domain-specific retrieval, strict governance, and optimized search performance.\n",
    "Implementation:\n",
    "\n",
    "- Create separate tables per knowledge type (e.g., Finance, Healthcare, Documentation).\n",
    "- Use a common schema across tables for consistency.\n",
    "- Maintain separate embedding indexes for efficient similarity search.\n",
    "\n",
    "Example Table Structures:\n",
    "1. Finance Knowledge Table (`llm.knowledge.finance`)\n",
    "\n",
    "| chunk_id(BIGINT) | document_text(STRING)                  | metadata (JSON)         | vector_embedding (ARRAY<Float>) |\n",
    "|----|-------------------------------|-------------------------|---------------------------------|\n",
    "| 1  | \"Stock market trends in 2024...\" | {\"category\": \"Finance\"} | [0.87, 0.45, ...]               |\n",
    "\n",
    "2. Tech Knowledge Table (`llm.knowledge.tech`)\n",
    "\n",
    "| chunk_id(BIGINT) | document_text(STRING)                | metadata (JSON)         | vector_embedding (ARRAY<Float>) |\n",
    "|----|------------------------------|-------------------------|---------------------------------|\n",
    "| 2  | \"Python's async features...\" | {\"category\": \"Tech\"}    | [0.65, 0.23, ...]               |\n",
    "\n",
    "Pros of Multiple Tables:\n",
    "- ‚úî Better performance ‚Äì Queries are optimized for domain-specific retrieval.\n",
    "- ‚úî Simplified governance ‚Äì Different access controls per table (e.g., finance team sees only finance knowledge).\n",
    "- ‚úî Domain-specialized retrieval ‚Äì RAG can query only relevant tables, improving accuracy.\n",
    "- ‚úî Parallel processing ‚Äì Queries run faster when searching only necessary datasets.\n",
    "- ‚úî Better for LangChain graphs ‚Äì Optimized for creating and managing LangChain graphs.\n",
    "\n",
    "Cons of Multiple Tables:\n",
    "- ‚ùå More complex maintenance ‚Äì Requires managing multiple table schemas and pipelines.\n",
    "- ‚ùå Increased storage overhead ‚Äì Some knowledge may exist in multiple tables, causing duplication.\n",
    "- ‚ùå Cross-domain retrieval is harder ‚Äì Combining multiple knowledge sources requires joining tables.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "023b7364-84d0-44a1-a2a8-31364a497e9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defining catalog and database\n",
    "catalog = \"llm\"\n",
    "database = \"rag\"\n",
    "\n",
    "# Create catalog and database if not exists\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {catalog}.{database}\")\n",
    "\n",
    "print(f\"Catalog {catalog} and database {database} created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f8611b0-853b-4d7f-9697-b4e5aedac7f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2. Creating a Volume for RAG in Databricks\n",
    "---\n",
    "\n",
    "A **Volume** in Databricks serves as a **storage layer** for **unstructured and semi-structured knowledge sources** used in **Retrieval-Augmented Generation (RAG)**. It allows efficient management of **documents, embeddings, and reference materials** that LLMs retrieve during inference.\n",
    "\n",
    "A **Volume** is part of **Unity Catalog**, ensuring **governance, security, and metadata tracking** for knowledge sources. It supports:\n",
    "- **Raw textual documents** (PDFs, Markdown, JSON, CSV, etc.)  \n",
    "- **Preprocessed embeddings** (stored in Parquet or Delta format)  \n",
    "- **Reference images, code snippets, or domain-specific data**  \n",
    "\n",
    "### **Key Features of Volumes for RAG**\n",
    "1. **Organized Knowledge Storage**  \n",
    "   - Volumes store **retrievable knowledge sources** such as text documents, embeddings, and metadata in a **Databricks-managed** environment.\n",
    "   - **Tables are not stored in volumes**; instead, raw data is stored here before being indexed into vector stores or Delta tables.\n",
    "\n",
    "2. **Managed by Unity Catalog**  \n",
    "   - Unity Catalog provides **governance, access control, and security** for RAG knowledge sources.\n",
    "   - Enables **fine-grained permissions** to control who can retrieve different types of knowledge.\n",
    "\n",
    "3. **Supports Multi-Format Knowledge Storage**  \n",
    "   - Stores **unstructured knowledge** (PDFs, Markdown, JSON) and **vector embeddings** in Parquet format.\n",
    "   - Ideal for handling **staging data, embeddings, and reference documents** in RAG pipelines.\n",
    "\n",
    "4. **Integration with Vector Search and LLM Workflows**  \n",
    "   - Allows **bulk loading of knowledge** into Delta tables for vector indexing.\n",
    "   - **Seamless connection** with **FAISS, CrhomaDB, or Databricks Vector Search** for efficient retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "### **How Volumes Enhance RAG in Databricks**\n",
    "‚úÖ **Efficient Knowledge Storage** ‚Äì Organizes raw **textual and embedding data** for LLM retrieval.  \n",
    "‚úÖ **Governed Access** ‚Äì Ensures **secure** knowledge management via **Unity Catalog**.  \n",
    "‚úÖ **Supports Hybrid RAG** ‚Äì Stores **text and multimodal data (images, PDFs, code, etc.)** for multi-source LLM retrieval.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Example: Storing Knowledge Sources in a Volume**\n",
    "#### **1Ô∏è‚É£ Creating a Volume**\n",
    "```sql\n",
    "CREATE VOLUME llm.knowledge.source_files;\n",
    "```\n",
    "\n",
    "#### **2Ô∏è‚É£ Uploading RAG Knowledge Files**\n",
    "```python\n",
    "dbutils.fs.cp(\"file:/local/path/documents.pdf\", \"dbfs:/Volumes/llm/knowledge/source_files/\")\n",
    "```\n",
    "\n",
    "#### **3Ô∏è‚É£ Reading Knowledge Files for Processing**\n",
    "```python\n",
    "documents = spark.read.text(\"dbfs:/Volumes/llm/knowledge/source_files/\")\n",
    "documents.show()\n",
    "```\n",
    "\n",
    "#### **4Ô∏è‚É£ Converting Knowledge into a Delta Table for Vector Indexing**\n",
    "```python\n",
    "documents.write.format(\"delta\").saveAsTable(\"llm.knowledge.document_embeddings\")\n",
    "```\n",
    "\n",
    "By using **Volumes in Databricks**, we enable **scalable, secure, and optimized knowledge storage** for **RAG-based LLM pipelines**, ensuring that retrieval is both **fast and reliable**. üöÄ  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6513a130-ca76-4473-b1c8-bee3fdcdfe3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define external location for the volume\n",
    "external_location = \"s3://silveraiwolf/landingzone/llm/rag/\"\n",
    "volume_path = f\"{catalog}.{database}.source_files\"\n",
    "\n",
    "# Create external volume if not exists\n",
    "spark.sql(f\"CREATE EXTERNAL VOLUME IF NOT EXISTS {volume_path} LOCATION '{external_location}'\")\n",
    "\n",
    "print(f\"External volume {volume_path} created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ac2a8a83-9959-4154-8022-1a08dae227b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3. Setting Up GitIngest\n",
    "---\n",
    "\n",
    "GitIngest enables pulling raw data from repositories directly into the workspace for ETL.\n",
    "\n",
    "1. Open a web browser and go to [GitIngest Repository](https://gitingest.com/samlexrod/silveraiwolf-learning).\n",
    "\n",
    "2. Locate and click on the **Download** button.\n",
    "\n",
    "3. Save the downloaded file to a preferred directory for further processing.\n",
    "\n",
    "4. Upload the downloaded `.txt` document into the previously created repository.\n",
    "\n",
    "> üéÅ Feel free to use the https://gitingest.com/samlexrod/silveraiwolf-learning repository for this tutorial. It is open-source and it will always be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a76656da-5885-414d-9dbc-b8b9de975c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 4. Create Base Tables\n",
    "---\n",
    "Enabling Change Data Capture (CDC) in base Delta tables allows efficient tracking of data changes over time. Here‚Äôs why it‚Äôs important:\n",
    "\n",
    "- **Incremental Updates** ‚Äì Instead of reprocessing the entire dataset, CDC enables tracking only the new, modified, or deleted records, improving performance and reducing compute costs.\n",
    "- **Data Lineage & Auditing** ‚Äì CDC logs changes over time, making it easier to trace data history, track modifications, and support compliance requirements.\n",
    "- **Real-time Processing** ‚Äì Delta tables with CDC integrate seamlessly with streaming workloads, allowing for near real-time updates in analytical or machine learning applications.\n",
    "- **Optimized for RAG Pipelines** ‚Äì Since LLMs often work with evolving datasets, CDC helps in efficiently updating knowledge bases without redundant processing.\n",
    "- **Improved Scalability** ‚Äì Large datasets benefit from incremental ingestion rather than full reloads, reducing the impact on storage and computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29008f9-dff5-4818-9d56-1fb6f7b83bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define knowledge table name\n",
    "table_name = \"knowledge_base\"\n",
    "\n",
    "# Create Delta table with Change Data Capture (CDC) enabled\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {catalog}.{database}.{table_name} (\n",
    "    id BIGINT GENERATED ALWAYS AS IDENTITY,\n",
    "    file_name STRING NOT NULL,\n",
    "    file_url STRING NOT NULL,\n",
    "    chunk_id INTEGER NOT NULL,\n",
    "    content STRING NOT NULL,\n",
    "    content_type STRING NOT NULL,\n",
    "    total_file_chunks INTEGER NOT NULL,\n",
    "    updated_by STRING,\n",
    "    updated_at TIMESTAMP,\n",
    "    inserted_at TIMESTAMP DEFAULT current_timestamp\n",
    ")\n",
    "USING DELTA\n",
    "TBLPROPERTIES (delta.enableChangeDataFeed=true, delta.feature.allowColumnDefaults='supported')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30b1de2a-f7a1-454e-842d-36480f33a19c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 5. Incremental *.txt ETL with SQL Merge\n",
    "\n",
    "This section walks through a full data pipeline that processes raw text files, splits them into chunks, and merges the structured data into a Delta Table. The main goals of this process are:\n",
    "\n",
    "- ‚úÖ Text Processing: Read and split large text files into manageable chunks for better retrieval.\n",
    "- ‚úÖ Schema Structuring: Maintain metadata such as file_name, chunk_id, and total_file_chunks.\n",
    "- ‚úÖ Efficient Storage & Retrieval: Store the processed data in a Delta Table for efficient querying.\n",
    "- ‚úÖ Change Tracking: Use MERGE INTO to update, insert, or delete records dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beb3b161-f70b-4a8e-ab10-ad68093ecee5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a UDF to process the chunks\n",
    "@udf(ArrayType(StringType()))\n",
    "def process_chunks(content: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits the text into chunks. \n",
    "\n",
    "    Parameters:\n",
    "    content (str): The text to be split into chunks.\n",
    "\n",
    "    Returns:\n",
    "    list[str]: A list of chunks.\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=1000, \n",
    "    chunk_overlap=200, # Important to have overlap to avoid splitting on important sentences\n",
    "    length_function=len\n",
    "    )\n",
    "    chunks = splitter.split_text(content)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# Repeated variable to clarify that we are using the same path as above\n",
    "directory_path = \"/Volumes/llm/rag/source_files/raw/\"\n",
    "\n",
    "# PROCESSING NEW FILES\n",
    "# Filtering the files that are not in the processed_files list\n",
    "bullet_format = lambda x: f\" - {x}\"\n",
    "new_files = os.listdir(directory_path)\n",
    "\n",
    "# Printing the list of new files\n",
    "print(f\"New files: \\n{',\\n'.join(map(bullet_format, new_files))} \\n\")\n",
    "\n",
    "# Template for the knowledge base table\n",
    "schema = StructType([\n",
    "    StructField(\"file_name\", StringType(), True),\n",
    "    StructField(\"chunk_id\", IntegerType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"content_type\", StringType(), True),\n",
    "    StructField(\"total_file_chunks\", IntegerType(), True)\n",
    "])\n",
    "df_knoledge_base = spark.createDataFrame([], schema)\n",
    "column_order = df_knoledge_base.columns\n",
    "\n",
    "for file_name in new_files:\n",
    "    txt_path = os.path.join(directory_path, file_name)\n",
    "\n",
    "    # Read the file\n",
    "    file_text =  open(txt_path, 'r').read()\n",
    "\n",
    "    # Specific case parsing\n",
    "    if file_name == \"samlexrod-silveraiwolf-learning.txt\":\n",
    "        \"\"\"\n",
    "        The following pattern splits the gitingest sections per file so the rag can remain in context within the document.\n",
    "\n",
    "        E.g. Each one of the sections are separated by a pattern:\n",
    "        ================================================\n",
    "        File: README.md\n",
    "        ================================================\n",
    "        # Content\n",
    "        \"\"\"\n",
    "\n",
    "        content_type = \"Git Repo\"\n",
    "        base_url = \"https://github.com/samlexrod/silveraiwolf-learning/blob/master/\"\n",
    "\n",
    "        # Pattern specific for gitingest (=== pattern)\n",
    "        pattern = r\"={48}\\nFile: (.+?)\\n={48}\\n\"\n",
    "\n",
    "        # Splitting the text while keeping filenames\n",
    "        sections = re.split(pattern, file_text)\n",
    "        \n",
    "        directory_section = sections[0]\n",
    "        file_sections = sections[1:]  # First element is empty, so we start from index 1\n",
    "\n",
    "        # Organizing data into dictionary {filename: content}\n",
    "        file_dict = {file_sections[i]: file_sections[i+1].strip() for i in range(0, len(file_sections), 2) if \"tutorial-data\" not in file_sections[i]}\n",
    "        file_dict[\"directory\"] = directory_section\n",
    "\n",
    "        # Converting dictionary to list of tuples\n",
    "        data = [(file_name, content) for file_name, content in file_dict.items()]\n",
    "\n",
    "        # Defining schema\n",
    "        schema = StructType([\n",
    "            StructField(\"file_name\", StringType(), True),\n",
    "            StructField(\"content\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        # Creating a dataframe for Delta table\n",
    "        df_chunks = (spark.createDataFrame(data, schema)\n",
    "            .withColumn(\"content\", process_chunks(col(\"content\"))) \n",
    "            .withColumn(\"total_file_chunks\", array_size(col(\"content\")))\n",
    "            .select(\n",
    "                \"file_name\", \n",
    "                \"total_file_chunks\",\n",
    "                posexplode(col(\"content\")).alias(\"chunk_id\", \"content\"))\n",
    "            .withColumn(\"chunk_id\", col(\"chunk_id\") + 1)\n",
    "            .select(*column_order)\n",
    "        )\n",
    "\n",
    "        # Adding content_type column \n",
    "        df_chunks = df_chunks.withColumn(\"content_type\", lit(content_type))\n",
    "\n",
    "        # Adding file_url column for the rag to use for the link\n",
    "        df_chunks = df_chunks.withColumn(\"file_url\", concat(lit(base_url), col(\"file_name\")))\n",
    "\n",
    "    else:\n",
    "        pass # Other file logic not in the scope of this tutorial\n",
    "\n",
    "    # Unioning the chunks into the knowledge base table\n",
    "    df_knoledge_base = df_knoledge_base.union(df_chunks)\n",
    "\n",
    "# Creating a view for merge\n",
    "df_knoledge_base.createOrReplaceTempView(\"knowledge_base_view\")\n",
    "\n",
    "# Merge data into Delta table\n",
    "spark.sql(f\"\"\"\n",
    "MERGE INTO {catalog}.{database}.{table_name} AS target\n",
    "USING knowledge_base_view AS source\n",
    "ON target.file_name = source.file_name AND target.chunk_id = source.chunk_id\n",
    "WHEN MATCHED AND target.content != source.content or coalesce(target.file_url, '') != source.file_url THEN \n",
    "    UPDATE SET \n",
    "        file_url = source.file_url,\n",
    "        content = source.content, \n",
    "        total_file_chunks = source.total_file_chunks, \n",
    "        updated_by = current_user(),\n",
    "        updated_at = current_timestamp()\n",
    "WHEN NOT MATCHED THEN INSERT (file_name, file_url, chunk_id, content, total_file_chunks) VALUES (file_name, file_url, chunk_id, content, total_file_chunks)\n",
    "WHEN NOT MATCHED BY SOURCE THEN DELETE\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "913f8704-c223-4dea-b5ad-5c67126b9ff2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 6. Display Kowledge Base Table\n",
    "\n",
    "Now that you've structured the knowledge_base table, the next step is to integrate vector search into Databricks. This will allow efficient semantic retrieval of relevant text chunks for Retrieval-Augmented Generation (RAG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd6e15cb-6671-4f31-bd5d-ba849ef9a6f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_knoledge_base.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718ddf4a-32dd-4d2a-b64a-4ac30c306d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM llm.rag.knowledge_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3953ce69-6f80-4f2d-b5a8-48b75907cf20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2129582593862512,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "t1-databricks-gitingest-chunking-etl",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
